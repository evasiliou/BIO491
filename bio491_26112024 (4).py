# -*- coding: utf-8 -*-
"""BIO491_26112024.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Yad3HzTckGlO07izcUz_8p5sfjizV10Y

#BIO491

#Packages Installation
"""

!pip install gprofiler-official
!pip install requests
!pip install igraph
!pip install matplotlib
!pip install uniprot-id-mapper
!pip install BeautifulSoup4
!pip install 'matplotlib-venn[shapely]'

"""#Functions"""

import pandas as pd
import csv
import numpy as np
from gprofiler import GProfiler
from ast import literal_eval
import requests
import igraph as ig
import matplotlib.pyplot as plt
from matplotlib_venn import venn2, venn2_circles
from UniProtMapper import ProtMapper
import time
from bs4 import BeautifulSoup
import re


not_found_file = 'not_found_proteins.txt' #create a file that will save the proteins not found
with open (not_found_file, 'w') as file: #just to create the file
  file.write('')

# def fetching (total_size):

#   # Define total number of items and batch size
#   total_items = total_size
#   batch_size = 500
#   fetched_items = 0  # Counter to track progress

#   while fetched_items < total_items: #i tried to run while fetched_items <= total_items, in order to take the last item of the total proteins (e.g. all_proteins[:total_items])
#   #but i fall into an infinite loop
#       # Calculate the next batch size
#       remaining_items = total_items - fetched_items
#       current_batch_size = min(batch_size, remaining_items)

#       # Simulate the data fetching (replace with your fetch function)
#       # print(f"Fetching batch: {fetched_items + 1} to {fetched_items + current_batch_size}...")
#       time.sleep(0.5)  # Simulating delay for each fetch; remove this line in actual fetching

#       # Update the count of fetched items
#       fetched_items += current_batch_size

#       # Calculate and display progress
#       progress_percentage = (fetched_items / total_items) * 100
#       # print(f"Progress: {fetched_items}/{total_items} ({progress_percentage:.2f}%)\n")

#   print("\n All items fetched!")



def protein_to_gene_conversion(query_list):
    '''
    The protein_to_gene_conversion function takes as argument a list with uniprot ids and returns a dictionary
    that has as key the uniprot id and as value a dictionary with equivalent gene name {'Uniprot id': {'Gene': Gene name}}.
    '''
    mapper = ProtMapper()

    query = ','.join(query_list)

    from_db = 'UniProtKB_AC-ID'
    to_db = 'Gene_Name'
    ids = query


    results_df, failed_list  = mapper.get(ids=ids, from_db=from_db, to_db=to_db) #mapper.get returns a tuple with a df of the results, and a list with the ids that were not found
    #just a note: the failed list contains all the ids in a single string e.g ['P49913,P27695,P0DMM9']

    # fetching(len(query_list))

    dictionary = {}
    for i in range (len(results_df)):
      dictionary[results_df.loc[i, 'From']] = {'Gene': results_df.loc[i, 'To']}

    print ('\n Converted Proteins:', len(dictionary))
    print (dictionary)

    element = failed_list[0] #the element is a string that contains all the query proteins (e.g. 'P49913,P27695,P0DMM9')
    all_proteins = element.split(',') #this is a list


    # print (all_proteins)
    print ('\n All proteins:', len(all_proteins))

    common_proteins = list (set(all_proteins).intersection(set(dictionary.keys())))
    print ('\n Common Proteins:', len(common_proteins))

    not_found= []

    for protein in all_proteins:
      if protein not in common_proteins:
        not_found.append(protein)
        dictionary[protein] = {'Gene': 'None'}
        with open ('not_found_proteins.txt', 'a') as file: #append the proteins that were not found in the 'not_found_proteins.tsv' file
          if protein not in 'not_found_proteins.txt':
            file.write(f'%s\n' % (protein))

    print ('\n Not found:', len(not_found))


    return dictionary #returns a dictionary

def gene_to_protein_conversion(query_list):

    '''
    The gene_to_protein_conversion function takes as argument a list with gene names and returns a dictionary
    that has as key the gene name and as value a dictionary with equivalent protein id {'Gene name': {'Protein': Uniprot id}}.
    '''
    count_none = 0
    gp = GProfiler(
        user_agent='ExampleTool',  # optional user agent
        return_dataframe=True  # return pandas dataframe or plain python structures
    )
    print("\n Total proteins before conversion: ", len(query_list))

    # convert the uniprot ids to the equivalent gene accession numbers. the results are saved in a df
    # query_genes = ','.join(query_list)

    converted_df = gp.convert(organism='hsapiens',
                              query=query_list,
                              target_namespace='UNIPROTSWISSPROT_ACC')

    print(converted_df.head().to_string())
    # print(converted_df)

    # fetching (len(query_list))
    dictionary = {}

    for i in range(len(converted_df)):
        gene = converted_df.loc[i, 'incoming']
        protein = converted_df.loc[i, 'converted']

        if protein != 'None':
            if gene not in dictionary.keys():
                dictionary[gene] = {'Protein': protein}
        else:
            dictionary[gene] = {'Protein': 'None'}
            count_none += 1
            with open ('not_found_proteins.txt', 'a') as file: #append the proteins that were not found in the 'not_found_proteins.tsv' file
              if protein not in 'not_found_proteins.txt':
                file.write(f'%s\n' % (protein))



        # if protein not in dictionary.keys():
        #     if gene != 'None':
        #         dictionary[protein] = {'Gene': gene}
        #     else:
        #         dictionary[protein] = {'Gene': gene}
        #         count_none += 1
        # if protein not in dictionary.keys():
        #     dictionary[protein] = {'Gene':gene}

    incoming_proteins = list(converted_df.loc[:, 'incoming'])
    print ('\n Incoming_proteins:', len(incoming_proteins))
    print("\n Total proteins after conversion: ", (len(dictionary.keys())-count_none))
    print ("\n Total unconverted proteins: ", count_none)
    print ('Dictionary', dictionary)


    return dictionary #returns a dictionary

def gene_enrichment(dictionary, filename):

    '''
    The gene_enrichment function takes as arguments a dictionary (describe what this dictionary contains) and the name of the file where
    the results of the gene enrichment analysis will be saved.
    '''

    #query_genes = protein_to_gene_conversion(dictionary) #this is a list
    query_genes = []
    for uniprot_id in dictionary.keys():
        gene = dictionary[uniprot_id]['Gene']
        query_genes.append(gene)

    gp = GProfiler(
        user_agent='ExampleTool',  # optional user agent
        return_dataframe=True  # return pandas dataframe or plain python structures
    )

    query = gp.profile(organism='hsapiens',
                       query=query_genes,
                      #  sources=['GO:MF'],
                       sources=["GO:MF", "GO:CC", "GO:BP", "KEGG", "REAC", "WP", "HPA", "CORUM", "HP"],
                       # sources=["GO:MF", "GO:CC", "GO:BP"],
                       no_evidences=False)
    #
    print("\n Total proteins after enrichment: ", len(query))
    #print("\n Enrichment Results: \n", query.head().to_string())
    # # print (dictionary)
    # #print(list(query.columns.values))
    #
    go_terms = list((query.loc[:, "native"]))  # pathways with their unique accession number
    print('\n Total GO terms:', len(go_terms))
    name = list((query.loc[:, 'name']))  # a description of the pathway
    print('\n Total names:', len(name))
    p_value = list(query.loc[:, "p_value"])
    print('\n Total p-values:', len(p_value))
    intersections = list(query.loc[:, "intersections"])  # a list with the group of proteins that participate in the equivalent pathway
    print('\n Total intersections:', len(intersections))
    #print(intersections)  # prints a list of lists
    #
    # print("\n Total proteins after enrichment: ", len(dictionary))
    # # print (dictionary)
    column_labels = ['p-value', 'GO terms', 'Pathway(name)', 'Intersections'] #this is a list that contains column names for the csv file
    column_data = [p_value, go_terms, name, intersections] #this is a list that contains all the lists with the data that I want to include in the csv file
    new_csv_file(column_labels, column_data, filename, True)

def read_gene_enrichment_results(filename):
    '''
    The read_gene_enrichmenet_results function takes as argument the name of the file that contains the results from
    the gene enrichment analysis and returns a dictionary: 'go term' = {'p-value': p_value, 'Pathway(name)': pathway, 'Intersections': intersections}
    '''

    csv_file = filename
    df = pd.read_csv(csv_file, converters={'Intersections': literal_eval})  # the converter here is applied for the data under the 'Intersections' column
    # since they were saved as str and not as lists
    print(df.head().to_string())
    #print(df.columns.values)
    go_terms = list(df.loc[:, 'GO terms'])

    dictionary = {}
    for i in range(len(go_terms)):
        go_term = go_terms[i]
        p_value = float(df.loc[i, 'p-value'])
        pathway = (df.loc[i, 'Pathway(name)'])
        # print(cellular_process)
        # autophagy = bool(df.loc[i, 'Autophagy'])
        intersections = (df.loc[i, 'Intersections'])
        # print(intersection)
        dictionary[go_term] = {'p-value': p_value, 'Pathway(name)': pathway, 'Intersections': intersections}

    return (dictionary)
    # return (go_terms, p_value, cellular_process, intersections)

def new_csv_file (column_labels, column_data, filename, index_status):

    '''
    The new_csv_file function aims to create a new csv file using a dictionary which is then converted to a df.
    It takes as arguments, a list with the names of the column in the new file, a list of lists with the data under each column, and the name of the file.
    The index_status takes True or False boolean values and the user selects if the column index will be included or not at the new file.
    '''
    size = len(column_data[0])
    dictionary = {}
    for label, data in zip(column_labels, column_data):
        #print (data)
        dictionary[label] = data

    df=pd.DataFrame(dictionary, index = range(size))
    # print (df)

    df.to_csv(filename, index=index_status, sep=',')

    print ('\n The file', f'{filename}', 'has been saved!')

#the hippie_query function can be a backup in case that the hippie_api does not work
# def hippie_query (query_list,confidence_score):

#     '''
#     This function takes as argument a file that contains only the enriched genes (previously created). From this file takes the uniprot ids and after making the query
#     in hippie it finds the interactions of these proteins that were found experimentally. Then it prints a URL which when is clicked it downloads a tsv file with the results.
#     It also saves the results from hippie in a tsv file 'hippie_results_0.63.tsv' where '0.63' is the confidence score that was used for the analysis.
#     '''


#     hippie_interactions_df = pd.read_table('HIPPIE_all_db.tsv' ) #saves in a df all the interactions in hippie
#     # print (hippie_interactions_df)

#     hippie_query_pairs = [] #this is a list with tuples that has the pairs of all the hippie interactions
#     interactor_A = []
#     interactor_B = []
#     confidence_values_list = []

#     for i in range (len(hippie_interactions_df)):
#       confidence_value = float(hippie_interactions_df.loc[i, 'Confidence Value'])
#       # print (type(confidence_value))
#       interactor_a = hippie_interactions_df.loc[i, 'Gene Name Interactor A']
#       interactor_b = hippie_interactions_df.loc[i, 'Gene Name Interactor B']

#       if confidence_value >= float(confidence_score) and (interactor_a in query_list or interactor_b in query_list):
#         pair = (interactor_a, interactor_b)
#         if pair not in hippie_query_pairs:
#           hippie_query_pairs.append(pair)

#       interactor_A.append(interactor_a)
#       interactor_B.append(interactor_b)
#       confidence_values_list.append(confidence_value)

#     print ('\n Total HIPPIE interactions: ', len(hippie_query_pairs))
#     # print (len(hippie_query_pairs))


#     column_labels = ['Interactor A', 'Interactor B', 'Confidence Value']
#     column_data = [interactor_A, interactor_B, confidence_values_list]
#     filename = 'hippie_results.csv'
#     new_csv_file(column_labels, column_data, filename, True)


#     return hippie_query_pairs #returns a list of tuples that contains the results from the query

def hippie_query_api (query_list,confidence_score, filename):

    '''
    This function takes as argument a file that contains only the enriched genes (previously created). From this file takes the uniprot ids and after making the query
    in hippie it finds the interactions of these proteins that were found experimentally. Then it prints a URL which when is clicked it downloads a tsv file with the results.
    It also saves the results from hippie in a tsv file 'hippie_results_0.63.tsv' where '0.63' is the confidence score that was used for the analysis.
    '''

    #Just a note: the beautiful soup package was used with the help of ChatGPT
    url = 'http://cbdm-01.zdv.uni-mainz.de/~mschaefer/hippie/queryHIPPIE.php?'

    error_messages = {
        '400':' Bad Request: The request was invalid.',
        '414': 'Request-URI Too Long: The request URI is too long.',
        '401': 'Unauthorized: Authentication failed.',
        '404': 'Not Found: The requested resource does not exist.',
        '500': 'Internal Server Error: The server encountered an error.'
    }
    # print (query_list)

    query=';'.join(query_list)

    parameters = {
        'proteins':query,
        'layers': '1', #layers = 0 to query interactions within the input set or 1 to query interactions between the input set and HIPPIE (optional, default = 1)
        'conf_thres': confidence_score,
        'out_type': 'conc_file'
    }

    # else:
    #   for i in range(len(query_list)):
    #     batch_proteins = query_list[i:i+500]
    #     hippie_query_api(batch_proteins, confidence_score)


    response = requests.post(url,params=parameters) #make a query in HIPPIE using the uniprot ids from the file
    print ('\n Response:', response.status_code)
    if response.status_code == 200:

      soup = BeautifulSoup(response.content, 'html.parser')# Parse the HTML response to find the form
      form = soup.find('form', {'id': 'netQuery'})

      if form:
        # Extract form action URL and hidden input fields
        action_url = form.get('action')
        form_data = {input_tag.get('name'): input_tag.get('value') for input_tag in form.find_all('input')}
        print ('\n form_data:', form_data)
        print ('\n action_url:', action_url)
        full_action_url = requests.compat.urljoin(response.url, action_url)

        print(f"Submitting the next form to: {full_action_url}")

    # Step 4: Submit the form to download the file
        download_response = requests.post(full_action_url, data=form_data, stream=True)
        print ('Download HIPPIE results: ', response.url) #this is a link that downloads the tsv file when it is clicked - i suppose that it is the same link that they sent to us via email
        print ('Downlad_response:', download_response)

      else: #if form not found
        download_response = "No form found on the page."
        print("\n No form found on the page.")

    elif str(response.status_code) in error_messages.keys() :
      print ('\n %s: %s' % (response.status_code, error_messages[str(response.status_code)]))

    else: #if status code != 200 and there is not such response in error messages
      print ('\n Error:', response.status_code)

    # filename = 'hippie_results.tsv'
    #if I create a csv file to save my results, again it saves the query uniprot ids in an HTML format
    # urlretrieve(new_url, filename)
    with open(filename, 'w') as file:
        file.write(download_response.content.decode('utf-8'))


    # df = pd.DataFrame(download_response.content.decode('utf-8'))
    # # df.to_csv(filename, index=False, sep='\t')
    # print (df)


    print ('\n The file', f'{filename}', 'has been saved!')

    # read_hippie_results(filename)

def read_hippie_results (input_filename, output_filename):

  header = ['uniprot id 1', 'entrez gene id 1', 'gene name 1', 'uniprot id 2', 'entrez gene id 2', 'gene name 2', 'score']
  index = 0
  with open(input_filename, 'r') as file:
    reader = csv.reader(file, delimiter='\t')
    # print (list(reader))
    for i, row in enumerate(reader):
      if row != header:
        if row != []:
          protein = ((row[0]))# if i write print (row) it will print a list, so in this way it prints a str with a message with the protein which was not found
          pattern = "didn't find (.*) in the database, skipped it"

          matches = re.finditer(pattern, protein)
          for m in matches:
              protein = (m.group(1)) #m.group is a string

          with open ('not_found_proteins.txt', 'a') as file:
            if protein not in 'not_found_proteins.txt':
              file.write(f'%s\n' % (protein))
      else:
        index = i
        # print (index)
        break

  hippie_interactions_df = pd.read_table(input_filename, sep ='\t', skiprows = index)
  print (hippie_interactions_df.head().to_string())
  print (hippie_interactions_df.columns.values)


  hippie_interactions_df.dropna(inplace = True, subset = ['uniprot id 1', 'uniprot id 2']) #remove any rows with 'nan' values - note: 'nan' is considered as float

  #Prepare the columns for the creation of a new csv file
  confidence_values_list = list(hippie_interactions_df.loc[:, 'score'])
  # print (type(confidence_value))
  interactor_A_list_uniprot_ids = list(hippie_interactions_df.loc[:, 'uniprot id 1']) #discuss why i have chosen uniprot ids - i am not interested in genes but proteins + i have genes with diff names
  interactor_B_list_uniprot_ids = list(hippie_interactions_df.loc[:, 'uniprot id 2'])

  print ('\n Interactor A uniprot ids:', len (interactor_A_list_uniprot_ids))
  print (interactor_A_list_uniprot_ids)
  print ('\n Interactor B uniprot ids:', len (interactor_B_list_uniprot_ids))
  print (interactor_B_list_uniprot_ids)

  hippie_protein_pairs = []

  for a, b in zip (interactor_A_list_uniprot_ids, interactor_B_list_uniprot_ids):
    pair = (a, b) #just a note here: if i remove the pair (b,a) which is the same with (a,b) maybe i lose biderectional associations - useful (?) for networks
    if pair not in hippie_protein_pairs:
      hippie_protein_pairs.append(pair)

  #then i convert the uniprot ids to the equivalent gene names - just a note: The uniprot ids here are in the form of 'B2L11_HUMAN' and not accession numbers
  # interactor_A_list_genes = (protein_to_gene_conversion(interactor_A_list_uniprot_ids)['Gene'])

  dict_genes = protein_to_gene_conversion(interactor_A_list_uniprot_ids + interactor_B_list_uniprot_ids)
  # interactor_B_dict_genes = protein_to_gene_conversion(interactor_B_list_uniprot_ids)

  hippie_gene_pairs = []
  scores = []
  interactor_A_list_genes = []
  interactor_B_list_genes = []

  for pair in hippie_protein_pairs:
    protein_a = pair[0]
    protein_b = pair[1]
    index = hippie_protein_pairs.index(pair)

    if protein_a in dict_genes.keys() and protein_b in dict_genes.keys():
      gene_a = dict_genes[protein_a]['Gene']
      gene_b = dict_genes[protein_b]['Gene']

      if gene_a != 'None' and gene_b != 'None':
        pair = (gene_a, gene_b)

        if pair not in hippie_gene_pairs:
          hippie_gene_pairs.append(pair)
          interactor_A_list_genes.append(gene_a)
          interactor_B_list_genes.append(gene_b)
          scores.append(confidence_values_list[index])


  print ('\n Hippie gene pairs: ', len(hippie_gene_pairs))
  print ('\n Scores ', len(scores), '\n', scores)


  #create a new file that contains the interactors and the confidence value
  column_labels = ['Interactor A', 'Interactor B', 'Confidence Value']
  column_data = [interactor_A_list_genes, interactor_B_list_genes, scores]
  filename = output_filename
  new_csv_file(column_labels, column_data, filename, True)

  return hippie_gene_pairs #this is a list with tuples, each one represents a pair


def new_ppi_network(nodes, edges, localization, filename):
    pairs_list = []
    for pair in edges:
        # print (pair)
      if (pair[0] in nodes and pair[1] in nodes) and (pair[0] != 'None' and pair[1] != 'None'):
        query_protein_index = nodes.index(pair[0])  # pair is a tuple
        reference_protein_index = nodes.index(pair[1])  # pair is a tuple; the

        pairs_list.append((query_protein_index, reference_protein_index))
    dt = np.dtype('int', 'int')
    new_matrix = np.array(pairs_list, dtype=dt)

    # print(pairs_list)

    g = ig.Graph(pairs_list)
    # print(g)

    g.vs['nodes'] = nodes
    g.vs['localization'] = localization
    # g.es['protein'] = edges
    layout = g.layout("kamada_kawai")
    col_dict = {
        'cytosol': 'blue',
        'nucleus': 'green',
        'mitochondrion': 'yellow',
        'None': 'white'
    }
    g.vs['color'] = [col_dict[loc] for loc in g.vs['localization']]

    fig, ax = plt.subplots()
    ig.plot(g, layout=layout, vertex_label =g.vs['nodes'], vertex_size = 35, target=ax, bbox=(1000, 2000), margin=2250)

    # plt.figure(figsize=(10, 6))

    plt.savefig(filename)
    plt.show()

    print('\n The network', f'{filename}', 'has been created!')

def mobidb_query (query_list, input_type):

  url = 'https://mobidb.org/api/download_page?'

  error_messages = {
      '400':' Bad Request: The request was invalid.',
      '401': 'Unauthorized: Authentication failed.',
      '404': 'Not Found: The requested resource does not exist.',
      '414': 'Request-URI Too Long: The request URI is too long.',
      '500': 'Internal Server Error: The server encountered an error.'
  }

  query = ','.join(query_list)

  if input_type == 'genes':
    parameters = {
        'gene': query,
        'ncbi_taxon_id': '9606'
    }

  elif input_type =='proteins':
    parameters = {
        'acc': query,
        'ncbi_taxon_id': '9606'
    }

  response = requests.get (url, parameters)
  print ('\n Response:', response)
  response_dict = response.json()
  print ('\n Response json: ', response.json())


  dictionary = {}
  uniprot_id_list = []

  if response.status_code == 200:

    for i in range(len(response_dict['data'])):

      response_dict_data = response_dict['data'][i]
      gene_name = response_dict_data['gene']
      uniprot_id = response_dict_data['acc']
      protein_length = response_dict_data['length']

      if 'reviewed' in response_dict_data.keys():
        # localization = response_dict_data['localization']
        uniprot_id_list.append(uniprot_id)

        if 'curated-disorder-disprot' in response_dict_data.keys():
          disprot = response_dict_data['curated-disorder-disprot'] #from the dictionary of dictionaries i select only the dictionary that is realated to disprot. the disprot dictionary is a dictionary of dictionaries
          disprot_idr_content = disprot['content_fraction'] # this is the value of the content_fraction dictionary and contains the idr content
          # print (response_dict_data['localization'])

          #I noticed that some genes have also synonyms, so if i take the gene_name as it is in some cases i also take the synonyms (e.g. 'VCPSynonyms=HEL-220' )

          pattern = 'Synonyms'
          found = re.findall(pattern, gene_name)
          if found != []:

            matches = re.finditer(pattern, gene_name)
            for m in matches:
                index = m.start()

            gene_name = gene_name[:index]


          dictionary[uniprot_id] = {'Gene': gene_name, 'Disprot IDR Content':float(f'{(disprot_idr_content*100):2f}'), 'Length': protein_length}

        else:
          dictionary[uniprot_id] = {'Gene': gene_name, 'Disprot IDR Content': 'None', 'Length': protein_length} # it means that is not available/not found yet, so i did not put 0.0

        if 'localization' in response_dict_data.keys():
          localization = response_dict_data['localization']
          dictionary[uniprot_id]['Localization'] = localization
        else:
          dictionary[uniprot_id]['Localization'] = 'None'
          # with ('mobidb_not_found.txt', 'a') as file:
          #   file.write(f'{uniprot_id} \n')
          #   # print ('\n', uniprot_id)

  elif str(response.status_code) in error_messages.keys() :
    print ('\n %s: %s'% (response.status_code, error_messages[str(response.status_code)]))


  else:
    print ('\n Error:', response.status_code)

  print ('\n Mobidb Results:', dictionary)

  #prepare the columns for a csv file
  # column_labels = ['Uniprot ID', 'Gene', 'Disprot IDR Content', 'Length', 'Localization']
  # column_data = [uniprot_id_list,
  #               list(dictionary['Gene']),
  #               list(dictionary['Disprot IDR Content']),
  #               list(dictionary['Length']),
  #               list(dictionary['Localization'])]
  # new_csv_file(column_labels, column_data, filename, True)
  df = pd.DataFrame(dictionary)
  df.to_csv('mobidb_results.csv')

  return dictionary

def protein_interactors (pairs_list):

  '''
  This function takes as argument a list with protein pairs and returns a dictionary
  that has keys the protein ids and as values a list with proteins that each protein interacts with
  '''

  # counts_dict = {}  #this is a dict that has as keys the gene name and as values the number of interactors
  interactors_dict = {} #it has as key the protein id and as value a list with the protein ids of the proteins with which interacts
  pair_genes = []
  for pair in pairs_list:
    interactor_a = pair[0]
    interactor_b = pair[1]

    if interactor_a not in pair_genes:
      pair_genes.append (interactor_a)

    if interactor_b not in pair_genes:
      pair_genes.append (interactor_b)


  converted_genes_dict = gene_to_protein_conversion(pair_genes)
  print (converted_genes_dict)

  for pair in pairs_list:
    gene_a = pair[0]
    gene_b = pair[1]


    interactor_a = converted_genes_dict[gene_a]['Protein']
    interactor_b = converted_genes_dict[gene_b]['Protein']

    if interactor_a not in interactors_dict.keys():
      interactors_dict[interactor_a] = [interactor_b]
    elif interactor_b not in interactors_dict[interactor_a]:
      interactors_dict[interactor_a].append(interactor_b)

    if interactor_b not in interactors_dict.keys():
      interactors_dict[interactor_b] = [interactor_a] #maybe it counts itself
    elif interactor_a not in interactors_dict[interactor_b]:
      interactors_dict[interactor_b].append(interactor_a)

  return interactors_dict

"""#Data Processing"""

#-------------------------------------
#Remove all the autophagy-related proteins from the DisProt dataset by comparing these proteins to the ones downloaded from UniProt
#convert the tsv file with scores into a csv file
tsv_file = 'disprot.tsv'
disprot_df=pd.read_table(tsv_file, sep='\t')
print (disprot_df.head().to_string())

disprot_acc= []
for i in disprot_df.index:
    if disprot_df.loc[i,'acc'] not in disprot_acc: #create a list that contains all the idr proteins as in disprot DB with no duplicates
        disprot_acc.append(disprot_df.loc[i,'acc'])
print ('\n Total disprot proteins: ', len(disprot_acc))

#create a new file that contains only the uniprot ids without any duplicates
column_label = ['Uniprot ID']
column_data = [disprot_acc]
# column_data.append(disprot_acc)
# print(column_data)
filename = 'disprot_proteins.csv'
new_csv_file(column_label, column_data, filename, True)

#------------------------------------------

"""#Gene Enrichment"""

#Read the files for gene enrichment


#The file with the disprot accession numbers (all the proteins related to autophagy have been removed)
csv_file = 'disprot_proteins.csv' #the new file has been saved in csv format
df = pd.read_csv(csv_file)  # convert the csv file into a pandas dataframe
# print (df)
disprot_ids = list(df.loc[:, "Uniprot ID"])  # save the uniprot ids in a Series pandas data type
# print (uniprot_ids)

disprot_protein_genes_dict = protein_to_gene_conversion(disprot_ids) #returns a list
gene_enrichment(disprot_protein_genes_dict,'disprot_no_duplicates_gene_enrichment.csv' )


#Gene enrichment analysis is applied at the new file from UniProt (new query, only primary genes are applied)
#No check for duplicates is needed
#The file with uniprot proteins related to autophagy (reference file)

tsv_file = 'uniprot_new_query_only_primary_genes.tsv'
df=pd.read_table(tsv_file, sep='\t')
#print (list(df.columns.values))
uniprot_ids = list(df.loc[:, "Entry"])  # save the uniprot ids in a Series pandas data type
print ('\n Total proteins: ', len(uniprot_ids))

uniprot_gene_dict = {}
# for i in range (len(uniprot_ids)):
#    protein_gene_dict[df.loc[i,"Entry"]] = {"Gene": ''} #create a dict with keys the uniprot id in each row and values the equivalent score in each rowfor
#    protein_gene_dict[df.loc[i, "Entry"]]["Gene"] = (df.loc[i, 'Gene Names (primary)'])
#    #overall_scores[df.loc[i, 'Entry']] = df.loc[i, 'Gene Names (primary)']

uniprot_protein_genes_dict = protein_to_gene_conversion(uniprot_ids) #returns a list
print ('\n Total genes: ', len(uniprot_protein_genes_dict.values()))
gene_enrichment(uniprot_protein_genes_dict, 'uniprot_new_query_primary_genes_gene_enrichment.csv')

#------------------------------

#Find the common pathways after gene enrichment

#the read_gene_enrichemnt_results returns a dict like this: dictionary[go_term] = {'p-value': p_value, 'Pathway(name)': pathway,'Intersections': intersections}

uniprot_dict = read_gene_enrichment_results('uniprot_new_query_primary_genes_gene_enrichment.csv')#this is a dictionary
disprot_dict = read_gene_enrichment_results('disprot_no_duplicates_gene_enrichment.csv')#this is a dictionary

# print ('\n Uniprot dictionary: \n', uniprot_dict)
# print ('\n Disprot dictionary: \n', disprot_dict)

#filter the two dictionaries in order to take only the significant go terms meaning only the ones with p_value<=0.05
uniprot_significant = []
disprot_significant = []

for go_term in uniprot_dict.keys():
    p_value = uniprot_dict[go_term]["p-value"]
    if p_value<=0.05:
        uniprot_significant.append(go_term)
        #print (go_term, '|', p_value)
print ('\n Uniprot significant go terms: ', len(uniprot_significant))

for go_term in disprot_dict.keys():
    p_value = disprot_dict[go_term]["p-value"]
    if p_value<=0.05:
        disprot_significant.append(go_term)
        #print (go_term, '|', p_value)
print ('\n Disprot significant go terms: ', len(disprot_significant))
#the following code identifies the common GO terms and filters the dictionaries to retain only those GO terms as keys

common_go_terms = list(set(disprot_significant).intersection(set(uniprot_significant)))
print ('\n Common GO terms: ', len (common_go_terms))

uniprot_dict_keys = list(uniprot_dict.keys())
disprot_dict_keys = list(disprot_dict.keys())

for key in uniprot_dict_keys:
    if key not in common_go_terms:
        uniprot_dict.pop(key)

print ('\n Total uniprot go terms:', len(uniprot_dict))

for key in disprot_dict_keys:
    if key not in common_go_terms:
        disprot_dict.pop(key)

print('\n Total disprot go terms:', len(disprot_dict))

#the block below saves in the common_p_values list the p_values of each
#the block below searches each list of genes line by line to find common genes
#Just a note here: if the go terms are not necessary we can remove the common_dict and write a simpler code block

common_dict = {} #create a dictionary that contains only the common elements between the two datasets
for go_term in common_go_terms:
    common_dict[go_term] = []

#print (common_dict)
for go_term in common_go_terms:
    uniprot_genes = uniprot_dict[go_term]['Intersections'] #this is a list
    disprot_genes = disprot_dict[go_term]['Intersections'] #this is a list
    common_genes = set(uniprot_genes).intersection(set(disprot_genes)) #this is a set

    if common_genes != {}: # check if the result of the intersection is not empty
        common_dict[go_term] = list(common_genes)
    else:
        common_dict[go_term] = ['None']

    #common_dict[go_term]['Common Pathway'] = uniprot_dict[go_term]['Pathway(name)']

print ('\n Common genes:' , len (common_dict))
# print ('\n Common genes:' , len (common_dict), '\n', common_dict)


#create the rows for the new file
common_genes_list = []
#common_pathway= []
for go_term in common_go_terms:
    common_genes_list.append(common_dict[go_term])
    #common_cellular_process.append(common_dict[go_term]['Common Process'])
column_label = ['GO terms', 'Common Intersections']
column_data = [common_go_terms, common_genes_list]
filename = 'common_gene_enrichment.csv'
new_csv_file(column_label,column_data,filename,False)


#With the code below we create a new dictionary that has as keys the enriched genes and as values the go terms (maybe the processes are added later)

enriched_genes_dict = {}

for go_term, common_genes in common_dict.items ():
    for gene in common_genes:
        if gene != 'None' and gene not in enriched_genes_dict.keys():
            enriched_genes_dict[gene] = [go_term]
        else:
            enriched_genes_dict[gene].append(go_term)

print('\n Enriched genes:', len(list(enriched_genes_dict.keys())), '\n', list(enriched_genes_dict.keys()))

enriched_genes = list(enriched_genes_dict.keys())
print (enriched_genes)

enriched_proteins_dict = gene_to_protein_conversion(enriched_genes)
# print (enriched_proteins_dict)
enriched_proteins = []

for dictionary in enriched_proteins_dict.values():
    enriched_proteins.append(dictionary['Protein'])

print (uniprot_protein_genes_dict.keys())

"""#MultiUniRed Data Analysis"""

#Multiunired Analysis
#The code block below reads the file with the results from multiunired and saves them in a dict

tsv_file = 'disprot_output.tsv'
df=pd.read_table(tsv_file, sep='\t', index_col='Unnamed: 0')
df.drop('Overall_Score', axis = 1, inplace = True) # axis 1 are the columns, inplace means to change the existing df, so since i need only the protein names i remove the last column that contains the total score
# print (df)
#this file is a table where each row represents the query protein and each column the reference protein

multiunired_rows = list(df.index)  # saves the proteins in a list (the rows in MultiUnired analysis contain the proteins from DisProt)
# this is just to keep only the proteins from the new query since the maltiunired analysis was done using an older query in UniProt
multiunired_columns = list(set(df.columns.values).intersection(set(uniprot_protein_genes_dict.keys()))) #saves the proteins in a list (the columns in MultiUniRed analysis contain the proteins from UniProt)
for protein in list(df.columns.values):
  if protein not in multiunired_columns:
    df.drop(protein, axis = 1, inplace = True)

# multiunired_columns = list(df.columns.values)



print (multiunired_columns)
print ('\n MultiUniRed Rows:', len(multiunired_rows))
print ('\n MutliUniRed Columns:', len(multiunired_columns))
# print (uniprot_ids)

query_protein_gene_dict = protein_to_gene_conversion(multiunired_rows) #print the list with the equivalent genes
reference_protein_gene_dict = protein_to_gene_conversion(multiunired_columns)

#Filter the results from MultiUniRed and keep only the gene names (not the uniprot_ids) and remove every row/column that has as gene the value 'None' and save them in a new dataframe

#Rows Modification
rows_genes = []
for protein in multiunired_rows:
    if query_protein_gene_dict[protein]['Gene'] == 'None':
        df.drop(protein, axis = 0, inplace = True)
    else:
        gene = query_protein_gene_dict[protein]['Gene']
        rows_genes.append(gene)

print ('\n Rows genes:', len(rows_genes))

#Columns Modification
# Remove all the gene names = 'None'

columns_genes = []
for protein in multiunired_columns:
    if reference_protein_gene_dict[protein]['Gene'] == 'None':
        df.drop(protein, axis = 1, inplace = True)
    else:
        gene = reference_protein_gene_dict[protein]['Gene']
        columns_genes.append(gene)
# print (df)

print ('\n Columns genes:', len(columns_genes))

scores_array = df.values.astype(float) #create an that contains all the scores of the protein associations found from the maltiunired
print (scores_array)

multiunired_results_df = pd.DataFrame (scores_array, columns = columns_genes, index = rows_genes)
print (multiunired_results_df.head().to_string())
multiunired_results_df.to_csv('multiunired_converted_axis.csv') #save the updated matrix with the multiunired results

multiunired_interactions_list = []
#Method 1 to find multiunired interactions - takes a lot of time to run



# for i in range (len(multiunired_results_df.index)): #rows
#   for j in range (len(multiunired_results_df.columns)):
#     # if scores_matrix[i,j] == 1.0:
#     if multiunired_results_df.iloc[i,j] == 1.0:
#       # query_protein = rows_genes[i] #IDR protein
#       # reference_protein = columns_genes[j] #UniProt Protein
#       query_protein = multiunired_results_df.index[i]
#       reference_protein = multiunired_results_df.columns[j]
#       pair = (query_protein, reference_protein)
#       if pair not in multiunired_interactions_list:
#         multiunired_interactions_list.append(pair)

#Method 2 to find multiunired interactions
interactions_matrix = np.nonzero(scores_array==1.0 ) #this is a tuple of arrays : one for each dimension. # I decided to create a new matrix with the scores only equal to one
# The first array represents the row indices where these values are found, and the second array represents the column indices where the values are found.
print (interactions_matrix)
print (len(interactions_matrix))
# not_interacting_matrix = np.array(scores_array==0.0) #returns a False/True matrix

# print (not_interacting_matrix)

not_interacting_indices = scores_array[scores_array==0.0]

list_of_coordinates = list(zip(interactions_matrix[0],interactions_matrix[1])) # create a list of the coordinates by merging the two arrays--> we have a list of tuples
#interacted[0] are the query proteins, interacted[1] are the reference protein
print (list_of_coordinates)
print (len(list_of_coordinates))

for coord in list_of_coordinates: #coord is a tuple
     # print (coord)
    query_protein_index = int(coord[0]) # coord[0] is a class 'numpy.int64' so i transform it into an int
    reference_protein_index = int(coord[1])


    query_protein = rows_genes[query_protein_index] #i take the index of the matrix that the score is equal to one, and i
    #use this index to find the gene at this position in the rows_genes list
    reference_protein = columns_genes[reference_protein_index]
    # print (query_protein)

    pair = (query_protein, reference_protein) #if i take this kind of pairs i take 56564 pairs, 492 common with hippie
    # pair  = (reference_protein, query_protein) #if i take this kind of pairs i take 56564 pairs, 446 common with hippie

    # print (pair)
    if pair not in multiunired_interactions_list:
        multiunired_interactions_list.append(pair)

# print (multiunired_interactions_list)
print ('MultiUnired interactions: ', len (multiunired_interactions_list))

#save the pairs in a csv file
interactor_A = []
interactor_B = []
for pair in multiunired_interactions_list:
  interactor_A.append(pair[0])
  interactor_B.append(pair[1])

new_csv_file(column_labels = ['Interactor A', 'Interactor B'], column_data = [interactor_A, interactor_B], filename = 'multiunired_interactions.csv', index_status = False )

print (multiunired_interactions_list)

traf6 = []
becn1 = []
for pair in multiunired_interactions_list:
  interactor_a = pair[0]
  interactor_b = pair[1]

  if interactor_a == 'TRAF6' or interactor_b == 'TRAF6':
    traf6.append(pair)

    with open ('multiunired_TRAF6.txt', 'a') as file:
      file.write(f'{pair}\n')
  if interactor_a == 'BECN1' or interactor_b == 'BECN1':
    becn1.append(pair)
    with open ('multiunired_BECN1.txt', 'a') as file:
      file.write(f'{pair}\n')

print (set(traf6).intersection(set(becn1)))

if ('BECN1', 'TRAF6') in multiunired_interactions_list:
  print ('yes')

"""#HIPPIE QUERY"""

outfile_name = 'hippie_TRAF6.tsv'
hippie_query_api(['TRAF6'], 0.73, outfile_name)

# uniprot_protein_genes_dict = protein_to_gene_conversion(uniprot_ids) #returns a list
print (disprot_ids)
print (uniprot_ids)

print (len(disprot_ids))
print (len(uniprot_ids))

all_proteins = uniprot_ids+disprot_ids
print (all_proteins)
print (len(all_proteins))

# for i, query_genes in enumerate(all_proteins):
  # print (query_genes)
total_items = len(all_proteins)
batch_size = 250
fetched_items = 0  # Counter to track progress
count_files = 0 #count the totla number of the created files

while fetched_items < total_items:
      # Calculate the next batch size
      remaining_items = total_items - fetched_items
      current_batch_size = min(batch_size, remaining_items)

      batch_proteins = all_proteins[fetched_items : fetched_items + current_batch_size]

      outfile_name = 'hippie_' + str(count_files+1) +'.tsv'
      hippie_query_api(batch_proteins, 0.73, outfile_name)
      # Simulate the data fetching (replace with your fetch function)
      # print(f"Fetching batch: {fetched_items + 1} to {fetched_items + current_batch_size}...")
      time.sleep(0.5)  # Simulating delay for each fetch; remove this line in actual fetching

      # Update the count of fetched items
      fetched_items += current_batch_size

      # Calculate and display progress
      progress_percentage = (fetched_items / total_items) * 100
      print(f"Progress: {fetched_items}/{total_items} ({progress_percentage:.2f}%)\n")
      count_files+=1



print ('\n Total files created:', count_files)
  # batch_proteins = all_proteins[i:i+500]

total_hippie_pairs = []
excluded_files = [] #files that are excluded because they contain no results from HIPPIE analysis - this may occur because of an error (e.g. 500)

for i in range(count_files):
  print (i)
  input_filename = 'hippie_' + str(i+1) +'.tsv'
  output_filename = 'hippie_pairs_' + str(i+1) +'.csv'
  with open (input_filename, 'r') as tsvfile:
    file_content = tsvfile.read()
    if "No form found on the page." in file_content: #I could also just check if the file is empty
      excluded_files.append(input_filename)
      print ('\n No results returned in ', f'{input_filename}')
    else:
      hippie_pairs = (read_hippie_results(input_filename, output_filename))

  for pair in hippie_pairs:
    if pair not in total_hippie_pairs:
      total_hippie_pairs.append(pair)

print ('\n Total Hippie pairs: ', len(total_hippie_pairs))

#below i concatinate all the files with the results from hippie
files_list = []

for i in range(count_files):
  filename = 'hippie_pairs_' + str(i+1) +'.csv'
  files_list.append(filename)

files_list = list(set(files_list).difference(set(excluded_files)))
print (files_list)

df = pd.concat(map(pd.read_csv, files_list), ignore_index=True)
print(df)

new_csv_file(column_labels = ['Interactor A', 'Interactor B', 'Confidence Value'], column_data = [df['Interactor A'], df['Interactor B'], df['Confidence Value']], filename = 'hippie_total_pairs.csv', index_status = False )

df = pd.read_csv('hippie_total_pairs.csv')
# print (df.head().to_string())
total_hippie_pairs = []

for i in range (len(df)):
  interactor_a = df.loc[i,'Interactor A']
  interactor_b = df.loc[i,'Interactor B']


  pair = (interactor_a, interactor_b)
  if pair not in total_hippie_pairs:
    total_hippie_pairs.append(pair)
print (total_hippie_pairs)
print (len(total_hippie_pairs))

# hippie_not_multiunired = len(set(total_hippie_pairs).difference(set(multiunired_interactions_list)))
# multiunired_not_hippie = len(set(multiunired_interactions_list).difference(set(total_hippie_pairs))) #this is a list that contains gene pairs, each pair in a tuple format
hippie_and_multiunired = len(set(total_hippie_pairs).intersection(set(multiunired_interactions_list)))
print (hippie_and_multiunired)
hippie_size = len(total_hippie_pairs)
multiunired_size = len(multiunired_interactions_list)
plt.figure(figsize = (4,3))
v = venn2 (subsets = (hippie_size, multiunired_size, hippie_and_multiunired),
           set_labels = ('HIPPIE', 'MultiUniRed'),
           set_colors = ('green', 'blue'),)
plt.title ('Comparison of Protein Interactions')
plt.savefig ('Comparison of protein interactions (venn_diagram).png')

plt.show()

!zip 'hippie_results.zip' hippie*.csv

"""#MobiDB"""

all_proteins = multiunired_interactions_list + total_hippie_pairs #i decided to merge all the proteins in order to run the mobidb query just once
print (all_proteins)
total_interactors_dict = protein_interactors (all_proteins)
print (total_interactors_dict)
print (len(total_interactors_dict))

# all_proteins = multiunired_interactions_list + total_hippie_pairs
total_interactors_list = list (total_interactors_dict.keys())
total_mobidb_dict = {}
# for i, query_genes in enumerate(all_proteins):
  # print (query_genes)
total_items = len(total_interactors_list)
batch_size = 500
fetched_items = 0  # Counter to track progress
count_run_times = 0 #count the totla number of the created files

while fetched_items <= total_items:
      # Calculate the next batch size
      remaining_items = total_items - fetched_items
      current_batch_size = min(batch_size, remaining_items)

      batch_proteins = total_interactors_list[fetched_items:fetched_items + current_batch_size]
      # print (batch_proteins)
      # outfile_name = 'mobidb_' + str(count_files+1) +'.tsv'
      mobidb_dict = mobidb_query(batch_proteins, 'proteins')
      total_mobidb_dict.update(mobidb_dict)
      # print (mobidb_dict)
      # print (total_mobidb_dict)
      #
      # print(f"Fetching batch: {fetched_items + 1} to {fetched_items + current_batch_size}...")
      time.sleep(0.5)  # Simulating delay for each fetch; remove this line in actual fetching

      # Update the count of fetched items
      fetched_items += current_batch_size

      # Calculate and display progress
      progress_percentage = (fetched_items / total_items) * 100
      print(f"Progress: {fetched_items}/{total_items} ({progress_percentage:.2f}%)\n")
      count_run_times+=1

print ('\n Total times ran:', count_run_times)
print ('\n Total proteins:', len(total_mobidb_dict))
  # batch_proteins = all_proteins[i:i+500]
print (total_mobidb_dict)

print (len(total_mobidb_dict))
print (count_run_times)

#this is just to append the protein counts in the total mobidb dict
for protein in total_interactors_dict.keys():

  if protein in total_mobidb_dict.keys():
    total_mobidb_dict[protein]['Protein Counts'] = len(total_interactors_dict[protein])

"""#Graphs

##Venn Diagram
"""

hippie_not_multiunired = (set(total_hippie_pairs).difference(set(multiunired_interactions_list)))
multiunired_not_hippie = (set(multiunired_interactions_list).difference(set(total_hippie_pairs))) #this is a list that contains gene pairs, each pair in a tuple format
hippie_and_multiunired = (set(total_hippie_pairs).intersection(set(multiunired_interactions_list)))
print ('\n HIPPIE NOT MULTIUNIRED:', len(hippie_not_multiunired))
print ('\n MULTIUNIRED NOT HIPPIE:', len(multiunired_not_hippie))
print ('\n HIPPIE AND MULTIUNIRED:', len(hippie_and_multiunired))

hippie_size = len(total_hippie_pairs)
multiunired_size = len(multiunired_interactions_list)
common = len(hippie_and_multiunired)

plt.figure(figsize = (4,4))
v = venn2 (subsets = (hippie_size, multiunired_size, common),
           set_labels = ('HIPPIE', 'MultiUniRed'),
           set_colors = ('green', 'blue'),)
plt.title ('Comparison of Protein Interactions')
plt.savefig ('Comparison of protein interactions (venn_diagram).png')

plt.show()

print (list(hippie_and_multiunired))

"""##Scatter Plots

### HIPPIE AND MULTIUNIRED
"""

#This graph shows the percentage of the idr content in relation to the number of interactors
#(I hypothesized that the number of interactors increases as the percentage of the idr content increases too - False)
#Just a note: Some proteins do not have an idr content yet so i set the idr content equal to 0.0  - revise)
x_axis = []
y_axis = []
no_idr_content = []
count_none = 0
count_idrs = 0
total = len(total_mobidb_dict.keys())

for protein in total_mobidb_dict.keys():
  protein_counts = total_mobidb_dict[protein]['Protein Counts']
  idr_content = total_mobidb_dict[protein]['Disprot IDR Content']

  if idr_content != 'None': # or idr_content != float
    x_axis.append(idr_content)
    y_axis.append(protein_counts)
    count_idrs +=1
  else:
    no_idr_content.append(protein)
    count_none +=1

print ('\n Total proteins with IDR content: (%s/%s)' % (count_idrs, total))
print ('\n Total proteins without IDR content: (%s/%s) \n' % (count_none, total))

plt.figure(figsize=(30, 20))
plt.scatter(x_axis, y_axis)
z = np.polyfit(x_axis, y_axis, 1)
p = np.poly1d(z)
plt.plot(x_axis, p(x_axis), color = 'purple', linewidth = 3, label = (p))
plt.xlabel('Disprot IDR Content (%)', fontsize = 20)
plt.ylabel('Number of Interactors', fontsize = 20)
plt.title('Disprot IDR Content vs. Number of Interactors', fontsize = 20)
plt.xticks(fontsize=15)
plt.yticks(fontsize=15)
plt.savefig('Disprot IDR Content (%) vs. Number of Interactors (All, trendline).png')
plt.show()

# trend_line = np.polyfit(x_axis, y_axis, 1)
# p = np.poly1d(trend_line)

# plt.plot(x_axis, p(x_axis), "g--")

print (p) #prints the equation of the trendline

"""###MULTIUNIRED NOT HIPPIE"""

multiunired_not_hippie = list (set(multiunired_interactions_list).difference(set(total_hippie_pairs))) #this is a list that contains gene pairs, each pair in a tuple format
print ('\n MultiUnired not HIPPIE: ', len(multiunired_not_hippie))

with open ('multi_not_hippie_pairs.txt', 'w') as file:
  for pair in multiunired_not_hippie:
    file.write ('%s\t%s\n' % (pair[0],pair[1]))

multiunired_interactors_dict = protein_interactors (multiunired_not_hippie) #this is a dict that has as keys the uniprot and as values a list with the uniprot ids of the proteins that interacts with

multiunired_interactors_list = list (multiunired_interactors_dict.keys())
multiunired_mobidb_dict = {}
# for i, query_genes in enumerate(all_proteins):
  # print (query_genes)
total_items = len(total_interactors_list)
batch_size = 300
fetched_items = 0  # Counter to track progress
count_run_times = 0 #count the totla number of the created files

while fetched_items < total_items:
      # Calculate the next batch size
      remaining_items = total_items - fetched_items
      current_batch_size = min(batch_size, remaining_items)

      batch_proteins = multiunired_interactors_list[fetched_items :fetched_items + current_batch_size]
      # print (batch_proteins)
      # outfile_name = 'mobidb_' + str(count_files+1) +'.tsv'
      mobidb_dict = mobidb_query(batch_proteins, 'proteins')
      multiunired_mobidb_dict.update(mobidb_dict)
      # print (mobidb_dict)
      # print (total_mobidb_dict)
      #
      # print(f"Fetching batch: {fetched_items + 1} to {fetched_items + current_batch_size}...")
      time.sleep(0.5)  # Simulating delay for each fetch; remove this line in actual fetching

      # Update the count of fetched items
      fetched_items += current_batch_size

      # Calculate and display progress
      progress_percentage = (fetched_items / total_items) * 100
      print(f"Progress: {fetched_items}/{total_items} ({progress_percentage:.2f}%)\n")
      count_run_times+=1

print ('\n Total times ran:', count_run_times)
print ('\n Total proteins:', len(total_mobidb_dict))
  # batch_proteins = all_proteins[i:i+500]
print (total_mobidb_dict)
# multiunired_mobidb_dict = mobidb_query(multiunired_interactors_dict.keys(), 'proteins')


for protein in multiunired_interactors_dict.keys():

  if protein in multiunired_mobidb_dict.keys():
    multiunired_mobidb_dict[protein]['Protein Counts'] = len(multiunired_interactors_dict[protein])

print (multiunired_interactors_dict)
print (multiunired_mobidb_dict)

#This graph shows the proteins that have idr content in relation to the number of interactors
x_axis = []
y_axis = []
count_none = 0
count_idrs = 0
total = len(multiunired_mobidb_dict.keys())

for protein in multiunired_mobidb_dict.keys():
  protein_counts = multiunired_mobidb_dict[protein]['Protein Counts']
  idr_content = multiunired_mobidb_dict[protein]['Disprot IDR Content']

  if idr_content != 'None': # or idr_content != float
    x_axis.append(idr_content)
    y_axis.append(protein_counts)
    count_idrs +=1
  else:
    count_none +=1


print ('\n Total proteins with IDR content: (%s/%s)' % (count_idrs, total))
print ('\n Total proteins without IDR content: (%s/%s) \n' % (count_none, total))

plt.figure(figsize=(30, 20))
plt.scatter(x_axis, y_axis)
z = np.polyfit(x_axis, y_axis, 1)
p = np.poly1d(z)
plt.plot(x_axis, p(x_axis), color = 'purple', linewidth = 3, label = (p))
plt.xlabel('Disprot IDR Content (%)', fontsize = 20)
plt.ylabel('Number of Interactors', fontsize = 20)
plt.title('Disprot IDR Content vs. Number of Interactors', fontsize = 20)
plt.xticks(fontsize=15)
plt.yticks(fontsize=15)
plt.savefig('Disprot IDR Content (%) vs. Number of Interactors (Multi, trendline).png')
plt.show()

print (p)

"""###HIPPIE NOT MULTIUNIRED"""

hippie_not_multiunired = list (set(total_hippie_pairs).difference(set(multiunired_interactions_list)))
print ('\n HIPPIE not Multiunired: ', len(hippie_not_multiunired))

with open ('hippie_not_multi_pairs.txt', 'w') as file:
  for pair in hippie_not_multiunired:
    file.write ('%s\t%s\n' % (pair[0], pair[1]))

hippie_interactors_dict = protein_interactors (hippie_not_multiunired)
hippie_interactors_list = list (hippie_interactors_dict.keys())
hippie_mobidb_dict = {}
# for i, query_genes in enumerate(all_proteins):
  # print (query_genes)
total_items = len(hippie_interactors_list)
batch_size = 300
fetched_items = 0  # Counter to track progress
count_run_times = 0 #count the totla number of the created files

while fetched_items < total_items:
      # Calculate the next batch size
      remaining_items = total_items - fetched_items
      current_batch_size = min(batch_size, remaining_items)

      batch_proteins = hippie_interactors_list[fetched_items:fetched_items + current_batch_size]
      # print (batch_proteins)
      # outfile_name = 'mobidb_' + str(count_files+1) +'.tsv'
      mobidb_dict = mobidb_query(batch_proteins, 'proteins')
      hippie_mobidb_dict.update(mobidb_dict)
      # print (mobidb_dict)
      # print (total_mobidb_dict)
      #
      # print(f"Fetching batch: {fetched_items + 1} to {fetched_items + current_batch_size}...")
      time.sleep(0.5)  # Simulating delay for each fetch; remove this line in actual fetching

      # Update the count of fetched items
      fetched_items += current_batch_size

      # Calculate and display progress
      progress_percentage = (fetched_items / total_items) * 100
      print(f"Progress: {fetched_items}/{total_items} ({progress_percentage:.2f}%)\n")
      count_run_times+=1

print ('\n Total times ran:', count_run_times)
print ('\n Total proteins:', len(total_mobidb_dict))
  # batch_proteins = all_proteins[i:i+500]
print (hippie_mobidb_dict)
# multiunired_mobidb_dict = mobidb_query(multiunired_interactors_dict.keys(), 'proteins')


for protein in hippie_interactors_dict.keys():

  if protein in hippie_mobidb_dict.keys():
    hippie_mobidb_dict[protein]['Protein Counts'] = len(hippie_interactors_dict[protein])

print (hippie_interactors_dict)
print (hippie_mobidb_dict)

#This graph shows the percentage of the idr content in relation to the number of interactors
#(I hypothesized that the number of interactors increases as the percentage of the idr content increases too - False)
#Just a note: Some proteins do not have an idr content yet so i set the idr content equal to 0.0  - revise)
x_axis = []
y_axis = []
count_none = 0
count_idrs = 0
total = len(hippie_mobidb_dict.keys())

for protein in hippie_mobidb_dict.keys():
  protein_counts = hippie_mobidb_dict[protein]['Protein Counts']
  idr_content = hippie_mobidb_dict[protein]['Disprot IDR Content']

  if idr_content != 'None': # or idr_content != float
    x_axis.append(idr_content)
    y_axis.append(protein_counts)
    count_idrs +=1
  else:
    count_none +=1

print ('\n Total proteins with IDR content: (%s/%s)' % (count_idrs, total))
print ('\n Total proteins without IDR content: (%s/%s) \n' % (count_none, total))

plt.figure(figsize=(30, 20))
plt.scatter(x_axis, y_axis)
z = np.polyfit(x_axis, y_axis, 1)
p = np.poly1d(z)
plt.plot(x_axis, p(x_axis), color = 'purple', linewidth = 3, label = (p))
plt.xlabel('Disprot IDR Content (%)', fontsize = 20)
plt.ylabel('Number of Interactors', fontsize = 20)
plt.title('Disprot IDR Content vs. Number of Interactors', fontsize = 20)
plt.xticks(fontsize=15)
plt.yticks(fontsize=15)
plt.savefig('Disprot IDR Content (%) vs. Number of Interactors (HIPPIE, trendline).png')
plt.show()

print (p)

"""##Histogram"""

#the following code blocks splits the data from total_mobidb_dict into idrs and no idrs
idr_content_list = []
protein_counts_list = []
x_axis = []
y_axis = []
no_idr_content = []
idr_length = []
no_idr_length = []
count_none = 0
count_idrs = 0
total = len(total_mobidb_dict.keys())

for protein in total_mobidb_dict.keys():
  protein_counts = total_mobidb_dict[protein]['Protein Counts']
  idr_content = total_mobidb_dict[protein]['Disprot IDR Content']
  length = total_mobidb_dict[protein]['Length']

  if idr_content != 'None': # or idr_content != float
    idr_content_list.append(idr_content)
    protein_counts_list.append(protein_counts)
    idr_length.append(length)
    count_idrs +=1
  else:
    no_idr_content.append(protein)
    no_idr_length.append(length)
    count_none +=1

print (len(idr_length))
print (len(no_idr_length))

"""###Average protein length of the proteins without IDR"""

sum = 0
avg = 0.0
for i in range (len(no_idr_length)-1):
  protein_1 = no_idr_length[i]
  protein_2 = no_idr_length[i+1]
  min_protein_length = min(protein_1, protein_2)
  max_protein_length = max(protein_1, protein_2)
  sum+= protein_1
print ('\n Min protein:', min_protein_length)
print ('\n Max protein:', max_protein_length)
print ('\n Average:', sum/len(no_idr_length))

"""###Average protein length of the proteins with IDR"""

sum = 0
avg = 0.0
for i in range (len(idr_length)-1):
  protein_1 = idr_length[i]
  protein_2 = idr_length[i+1]
  min_protein_length = min(protein_1, protein_2, min_protein_length)
  max_protein_length = max(protein_1, protein_2, max_protein_length)
  sum+= protein_1
print ('\n Min protein:', min_protein_length)
print ('\n Max protein:', max_protein_length)
print ('\n Average:', sum/len(idr_length))

#Histogram of the frequency of the protein lenghts
protein_lengths = []
for protein in total_interactors_dict.keys():

  if protein in total_mobidb_dict.keys():
    protein_lengths.append(total_mobidb_dict[protein]['Length'])



# Define bin edges (e.g., for bins of 500)
bins = range(0, max_protein_length+1, 200)  # Bins: 0-500, 501-1000, ..., 4501-5000

# Create the histogram
plt.figure(figsize=(20, 15))
plt.hist(protein_lengths, bins=bins, edgecolor = 'blue' )

# Add labels and title
plt.xlabel('Protein Length (aa)', fontsize=14)
plt.ylabel('Frequency', fontsize=14)
plt.title('Histogram of Protein Lengths', fontsize=16)
plt.xticks(bins, fontsize=12, rotation = 30)
plt.yticks(fontsize=12)

# Save and show the plot
plt.savefig('Histogram.png')
plt.show()

"""##Box Plot"""

#I noticed that the majority of proteins have length between 200 and 400 aa so I wanted to count the average number of interactors
sum_idr = 0
sum_no_idr = 0
avg_idr = 0.0
avg_no_idr = 0.0
total_idr = 0
total_no_idr = 0
idr_counts = []
no_idr_counts = []

for protein in total_interactors_dict.keys():

  if protein in total_mobidb_dict.keys():
    length = (total_mobidb_dict[protein]['Length'])
    protein_counts = total_mobidb_dict[protein]['Protein Counts']
    idr_content = total_mobidb_dict[protein]['Disprot IDR Content']

    if length in range (200,401):

      if idr_content != 'None':
        idr_counts.append(protein_counts)
        sum_idr+= protein_counts
        total_idr+=1

      elif idr_content == 'None':
        no_idr_counts.append(protein_counts)
        sum_no_idr+= protein_counts
        total_no_idr+=1

      else:
        print (protein)


avg_idr = sum_idr/total_idr
print (sum_idr)
print (total_idr)
print (f'\n Average idr: {avg_idr:.2f}')
avg_no_idr = sum_no_idr/total_no_idr
print (sum_no_idr)
print (total_no_idr)
print (f'\n Average no idr: {avg_no_idr:.2f}')

# help (plt.boxplot)

#box plots for idr
# print (idr_content_list)
# print (idr_counts)
plt.figure(figsize = (20,10))
bx = plt.boxplot([idr_counts, no_idr_counts], vert= True, sym = 'None', labels = ['IDR content', 'No IDR content'] )
plt.ylabel ('Protein Counts')
plt.title ('Average Number of Interactors')
plt.savefig('Average Number of Interactors (boxplot).png')
# plt.boxplot(no_idr_counts, vert = True, sym = 'None')
plt.show()

print ((bx['means']))

#I just repeated the processes for a different protein length range
sum_idr = 0
sum_no_idr = 0
avg_idr = 0.0
avg_no_idr = 0.0
total_idr = 0
total_no_idr = 0

for protein in total_interactors_dict.keys():

  if protein in total_mobidb_dict.keys():
    length = (total_mobidb_dict[protein]['Length'])
    protein_counts = total_mobidb_dict[protein]['Protein Counts']
    idr_content = total_mobidb_dict[protein]['Disprot IDR Content']

    if length in range (400,601):

      if idr_content != 'None':
        sum_idr+= protein_counts
        total_idr+=1

      elif idr_content == 'None':
        sum_no_idr+= protein_counts
        total_no_idr+=1

      else:
        print (protein)


avg_idr = sum_idr/total_idr
print (sum_idr)
print (total_idr)
print (f'\n Average idr: {avg_idr:.2f}')
avg_no_idr = sum_no_idr/total_no_idr
print (sum_no_idr)
print (total_no_idr)
print (f'\n Average no idr: {avg_no_idr:.2f}')

!zip 'BIO491_Project.zip' *.*

"""##Other plots"""

#This graph shows the name of the protein with the idr percentage - the dataset is too large so the names of the proteins cannot be seen in the x axis

# x_axis = []
# y_axis = []
# count_none = 0
# count_idrs = 0
# total = len(multiunired_mobidb_dict.keys())

# for protein in multiunired_mobidb_dict.keys():
#   idr_content = multiunired_mobidb_dict[protein]['Disprot IDR Content']

#   if idr_content != 'None': # or idr_content != float
#     x_axis.append(protein)
#     y_axis.append(idr_content)
#     count_idrs +=1
#   else:
#     count_none +=1

# print ('\n Total proteins with IDR content: (%s/%s)' % (count_idrs, total))
# print ('\n Total proteins without IDR content: (%s/%s) \n' % (count_none, total))


# plt.figure(figsize=(30, 10))
# plt.bar(x_axis, y_axis)
# plt.xlabel('Protein Accession')
# plt.ylabel('Disprot IDR Content (%)')
# plt.title('Disprot IDR Content')
# plt.xticks(fontsize=15, rotation=45)
# plt.savefig('Disprot IDR Content (%) (Multi).png')
# plt.show()

# #This graph shows the name of the protein with the idr percentage

# x_axis = []
# y_axis = []
# count_none = 0
# count_idrs = 0
# total = len(hippie_mobidb_dict.keys())

# for protein in hippie_mobidb_dict.keys():
#   idr_content = hippie_mobidb_dict[protein]['Disprot IDR Content']

#   if idr_content != 'None': # or idr_content != float
#     x_axis.append(protein)
#     y_axis.append(idr_content)
#     count_idrs +=1
#   else:
#     count_none +=1

# print ('\n Total proteins with IDR content: (%s/%s)' % (count_idrs, total))
# print ('\n Total proteins without IDR content: (%s/%s) \n' % (count_none, total))

# plt.figure(figsize=(15, 5))
# plt.bar(x_axis, y_axis)
# plt.xlabel('Protein Accession')
# plt.ylabel('Disprot IDR Content (%)')
# plt.title('Disprot IDR Content')
# plt.xticks(fontsize=5, rotation=45)
# plt.savefig('Disprot IDR Content (%) (HIPPIE).png')
# plt.show()