# -*- coding: utf-8 -*-
"""Human Proteome

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1eGt6ZvZNfXffbwbOJcQQ5cnsY6vreuW9

#BIO491

#Packages Installation
"""

!pip install gprofiler-official
!pip install requests
!pip install igraph
!pip install matplotlib
!pip install uniprot-id-mapper
!pip install BeautifulSoup4
!pip install 'matplotlib-venn[shapely]'

"""#Functions"""

import pandas as pd
import csv
import numpy as np
from gprofiler import GProfiler
from ast import literal_eval
import requests
import igraph as ig
import matplotlib.pyplot as plt
from matplotlib_venn import venn2, venn2_circles
from UniProtMapper import ProtMapper
import time
from bs4 import BeautifulSoup
import re
import sklearn as sk
from sklearn import linear_model
from sklearn.linear_model import LinearRegression
import scipy.stats as stats
from scipy.stats import mannwhitneyu
from math import log
from math import log10


not_found_file = 'not_found_proteins.txt' #create a file that will save the proteins not found
with open (not_found_file, 'w') as file: #just to create the file
  file.write('')

def fetching (total_size):

  # Define total number of items and batch size
  total_items = total_size
  batch_size = 500
  fetched_items = 0  # Counter to track progress

  while fetched_items < total_items: #i tried to run while fetched_items <= total_items, in order to take the last item of the total proteins (e.g. all_proteins[:total_items])
  #but i fall into an infinite loop
      # Calculate the next batch size
      remaining_items = total_items - fetched_items
      current_batch_size = min(batch_size, remaining_items)

      # Simulate the data fetching (replace with your fetch function)
      # print(f"Fetching batch: {fetched_items + 1} to {fetched_items + current_batch_size}...")
      time.sleep(0.5)  # Simulating delay for each fetch; remove this line in actual fetching

      # Update the count of fetched items
      fetched_items += current_batch_size

      # Calculate and display progress
      progress_percentage = (fetched_items / total_items) * 100
      # print(f"Progress: {fetched_items}/{total_items} ({progress_percentage:.2f}%)\n")

  print("\n All items fetched!")


def uniprot_id_mapping (query_list, uniprot_acc): #in case that the uniprot id mapper does not work
  '''
    query_list: list of uniprot ids
    uniprot_acc: bool value
  '''
  if uniprot_acc == True:
    filename = 'idmapping_acc.csv'
  else:
    filename = 'idmapping_id.csv'

  df = pd.read_csv (filename)
  uniprot_ids = list (df['Uniprot ID'])
  gene_names = list (df['Gene Name'])

  # print (uniprot_ids)
  # print (gene_names)

  not_found = []
  dictionary = {}

  # print (query_list)

  for protein in query_list:
    if protein in uniprot_ids:
      index = uniprot_ids.index(protein)
      # print (index)
      gene = gene_names[index]
      dictionary[protein] = {'Gene': gene}

    else:
      dictionary[protein] = {'Gene': 'None'}
      not_found.append(protein)
      with open ('not_found_proteins.txt', 'a') as file: #append the proteins that were not found in the 'not_found_proteins.tsv' file
        if protein not in 'not_found_proteins.txt':
          file.write(f'%s\n' % (protein))


  print ('\n Converted Proteins:', len(dictionary))
  print (dictionary)

  print ('\n Not found:', len(not_found))


  return dictionary #returns a dictionary

def protein_to_protein_accession_conversion (query_list):

    mapper = ProtMapper()

    query = ','.join(query_list)

    from_db = 'UniProtKB_AC-ID'
    to_db = 'UniProtKB'
    ids = query


    results_df, failed_list  = mapper.get(ids=ids, from_db=from_db, to_db=to_db) #mapper.get returns a tuple with a df of the results, and a list with the ids that were not found
    #just a note: the failed list contains all the ids in a single string e.g ['P49913,P27695,P0DMM9']

    # fetching(len(query_list))

    dictionary = {}
    for i in range (len(results_df)):
      dictionary[results_df.loc[i, 'From']] = {'Acc': results_df.loc[i, 'Entry']}

    print ('\n Converted Proteins:', len(dictionary))
    print (dictionary)

    element = failed_list[0] #the element is a string that contains all the query proteins (e.g. 'P49913,P27695,P0DMM9')
    all_proteins = element.split(',') #this is a list


    # print (all_proteins)
    print ('\n All proteins:', len(all_proteins))

    common_proteins = list (set(all_proteins).intersection(set(dictionary.keys())))
    print ('\n Converted Proteins:', len(common_proteins))

    not_found= []

    for protein in all_proteins:
      if protein not in common_proteins:
        not_found.append(protein)
        dictionary[protein] = {'Acc': 'None'}
        with open ('not_found_proteins_acc.txt', 'w') as file: #append the proteins that were not found in the 'not_found_proteins.tsv' file
          if protein not in 'not_found_proteins_acc.txt':
            file.write(f'%s\n' % (protein))

    print ('\n Not found:', len(not_found))


    return dictionary #returns a dictionary

def protein_to_gene_conversion(query_list):
    '''
    The protein_to_gene_conversion function takes as argument a list with uniprot ids and returns a dictionary
    that has as key the uniprot id and as value a dictionary with equivalent gene name {'Uniprot id': {'Gene': Gene name}}.
    '''
    mapper = ProtMapper()

    query = ','.join(query_list)

    from_db = 'UniProtKB_AC-ID'
    to_db = 'Gene_Name'
    ids = query


    results_df, failed_list  = mapper.get(ids=ids, from_db=from_db, to_db=to_db) #mapper.get returns a tuple with a df of the results, and a list with the ids that were not found
    #just a note: the failed list contains all the ids in a single string e.g ['P49913,P27695,P0DMM9']

    # fetching(len(query_list))

    dictionary = {}
    for i in range (len(results_df)):
      dictionary[results_df.loc[i, 'From']] = {'Gene': results_df.loc[i, 'To']}

    print ('\n Converted Proteins:', len(dictionary))
    print (dictionary)

    element = failed_list[0] #the element is a string that contains all the query proteins (e.g. 'P49913,P27695,P0DMM9')
    all_proteins = element.split(',') #this is a list


    # print (all_proteins)
    print ('\n All proteins:', len(all_proteins))

    common_proteins = list (set(all_proteins).intersection(set(dictionary.keys())))
    print ('\n Converted Proteins:', len(common_proteins))

    not_found= []

    for protein in all_proteins:
      if protein not in common_proteins:
        not_found.append(protein)
        dictionary[protein] = {'Gene': 'None'}
        with open ('not_found_proteins.txt', 'a') as file: #append the proteins that were not found in the 'not_found_proteins.tsv' file
          if protein not in 'not_found_proteins.txt':
            file.write(f'%s\n' % (protein))

    print ('\n Not found:', len(not_found))


    return dictionary #returns a dictionary

def gene_to_protein_conversion(query_list):

    '''
    The gene_to_protein_conversion function takes as argument a list with gene names and returns a dictionary
    that has as key the gene name and as value a dictionary with equivalent protein id {'Gene name': {'Protein': Uniprot id}}.
    '''
    count_none = 0
    gp = GProfiler(
        user_agent='ExampleTool',  # optional user agent
        return_dataframe=True  # return pandas dataframe or plain python structures
    )
    print("\n Total proteins before conversion: ", len(query_list))

    # convert the uniprot ids to the equivalent gene accession numbers. the results are saved in a df
    # query_genes = ','.join(query_list)

    converted_df = gp.convert(organism='hsapiens',
                              query=query_list,
                              target_namespace='UNIPROTSWISSPROT_ACC')

    print(converted_df.head().to_string())
    # print(converted_df)

    # fetching (len(query_list))
    dictionary = {}

    for i in range(len(converted_df)):
        gene = converted_df.loc[i, 'incoming']
        protein = converted_df.loc[i, 'converted']

        if protein != 'None':
            if gene not in dictionary.keys():
                dictionary[gene] = {'Protein': protein}
        else:
            dictionary[gene] = {'Protein': 'None'}
            count_none += 1
            with open ('not_found_proteins.txt', 'a') as file: #append the proteins that were not found in the 'not_found_proteins.tsv' file
              if protein not in 'not_found_proteins.txt':
                file.write(f'%s\n' % (protein))


    incoming_proteins = list(converted_df.loc[:, 'incoming'])
    print ('\n Incoming_proteins:', len(incoming_proteins))
    print("\n Total proteins after conversion: ", (len(dictionary.keys())-count_none))
    print ("\n Total unconverted proteins: ", count_none)
    print ('Dictionary', dictionary)


    return dictionary #returns a dictionary

def gene_enrichment(dictionary, filename):

    '''
    The gene_enrichment function takes as arguments a dictionary (describe what this dictionary contains) and the name of the file where
    the results of the gene enrichment analysis will be saved.
    '''

    #query_genes = protein_to_gene_conversion(dictionary) #this is a list
    query_genes = []
    for uniprot_id in dictionary.keys():
        gene = dictionary[uniprot_id]['Gene']
        query_genes.append(gene)

    gp = GProfiler(
        user_agent='ExampleTool',  # optional user agent
        return_dataframe=True  # return pandas dataframe or plain python structures
    )

    query = gp.profile(organism='hsapiens',
                       query=query_genes,
                      #  sources=['GO:MF'],
                       sources=["GO:MF", "GO:CC", "GO:BP", "KEGG", "REAC", "WP", "HPA", "CORUM", "HP"],
                       # sources=["GO:MF", "GO:CC", "GO:BP"],
                       no_evidences=False)
    #
    print("\n Total genes after enrichment: ", len(query))
    #print("\n Enrichment Results: \n", query.head().to_string())
    # # print (dictionary)
    # #print(list(query.columns.values))
    #
    go_terms = list((query.loc[:, "native"]))  # pathways with their unique accession number
    print('\n Total GO terms:', len(go_terms))
    name = list((query.loc[:, 'name']))  # a description of the pathway
    print('\n Total names:', len(name))
    p_value = list(query.loc[:, "p_value"])
    print('\n Total p-values:', len(p_value))
    intersections = list(query.loc[:, "intersections"])  # a list with the group of proteins that participate in the equivalent pathway
    print('\n Total intersections:', len(intersections))
    #print(intersections)  # prints a list of lists
    #
    # print("\n Total proteins after enrichment: ", len(dictionary))
    # # print (dictionary)
    column_labels = ['p-value', 'GO terms', 'Pathway(name)', 'Intersections'] #this is a list that contains column names for the csv file
    column_data = [p_value, go_terms, name, intersections] #this is a list that contains all the lists with the data that I want to include in the csv file
    new_csv_file(column_labels, column_data, filename, True)

def read_gene_enrichment_results(filename):
    '''
    The read_gene_enrichmenet_results function takes as argument the name of the file that contains the results from
    the gene enrichment analysis and returns a dictionary: 'go term' = {'p-value': p_value, 'Pathway(name)': pathway, 'Intersections': intersections}
    '''

    csv_file = filename
    df = pd.read_csv(csv_file, converters={'Intersections': literal_eval})  # the converter here is applied for the data under the 'Intersections' column
    # since they were saved as str and not as lists
    print(df.head().to_string())
    #print(df.columns.values)
    go_terms = list(df.loc[:, 'GO terms'])

    dictionary = {}
    for i in range(len(go_terms)):
        go_term = go_terms[i]
        p_value = float(df.loc[i, 'p-value'])
        pathway = (df.loc[i, 'Pathway(name)'])
        # print(cellular_process)
        # autophagy = bool(df.loc[i, 'Autophagy'])
        intersections = (df.loc[i, 'Intersections'])
        # print(intersection)
        dictionary[go_term] = {'p-value': p_value, 'Pathway(name)': pathway, 'Intersections': intersections}

    return (dictionary)
    # return (go_terms, p_value, cellular_process, intersections)

def new_csv_file (column_labels, column_data, filename, index_status):

    '''
    The new_csv_file function aims to create a new csv file using a dictionary which is then converted to a df.
    It takes as arguments, a list with the names of the column in the new file, a list of lists with the data under each column, and the name of the file.
    The index_status takes True or False boolean values and the user selects if the column index will be included or not at the new file.
    '''
    size = len(column_data[0])
    dictionary = {}
    for label, data in zip(column_labels, column_data):
        #print (data)
        dictionary[label] = data

    df=pd.DataFrame(dictionary, index = range(size))
    # print (df)

    df.to_csv(filename, index=index_status, sep=',')

    print (f'\n The file {filename} has been saved!')

#the hippie_query function can be a backup in case that the hippie_api does not work
# def hippie_query (query_list,confidence_score):

#     '''
#     This function takes as argument a file that contains only the enriched genes (previously created). From this file takes the uniprot ids and after making the query
#     in hippie it finds the interactions of these proteins that were found experimentally. Then it prints a URL which when is clicked it downloads a tsv file with the results.
#     It also saves the results from hippie in a tsv file 'hippie_results_0.63.tsv' where '0.63' is the confidence score that was used for the analysis.
#     '''


#     hippie_interactions_df = pd.read_table('HIPPIE_all_db.tsv' ) #saves in a df all the interactions in hippie
#     # print (hippie_interactions_df)

#     hippie_query_pairs = [] #this is a list with tuples that has the pairs of all the hippie interactions
#     interactor_A = []
#     interactor_B = []
#     confidence_values_list = []

#     for i in range (len(hippie_interactions_df)):
#       confidence_value = float(hippie_interactions_df.loc[i, 'Confidence Value'])
#       # print (type(confidence_value))
#       interactor_a = hippie_interactions_df.loc[i, 'Gene Name Interactor A']
#       interactor_b = hippie_interactions_df.loc[i, 'Gene Name Interactor B']

#       if confidence_value >= float(confidence_score) and (interactor_a in query_list or interactor_b in query_list):
#         pair = (interactor_a, interactor_b)
#         if pair not in hippie_query_pairs:
#           hippie_query_pairs.append(pair)

#       interactor_A.append(interactor_a)
#       interactor_B.append(interactor_b)
#       confidence_values_list.append(confidence_value)

#     print ('\n Total HIPPIE interactions: ', len(hippie_query_pairs))
#     # print (len(hippie_query_pairs))


#     column_labels = ['Interactor A', 'Interactor B', 'Confidence Value']
#     column_data = [interactor_A, interactor_B, confidence_values_list]
#     filename = 'hippie_results.csv'
#     new_csv_file(column_labels, column_data, filename, True)


#     return hippie_query_pairs #returns a list of tuples that contains the results from the query

def hippie_query_api (query_list,confidence_score, filename):

    '''
    This function takes as argument a file that contains only the enriched genes (previously created). From this file takes the uniprot ids and after making the query
    in hippie it finds the interactions of these proteins that were found experimentally. Then it prints a URL which when is clicked it downloads a tsv file with the results.
    It also saves the results from hippie in a tsv file 'hippie_results_0.63.tsv' where '0.63' is the confidence score that was used for the analysis.
    '''
    hippie_query_api.counter +=1
    #Just a note: the beautiful soup package was used with the help of ChatGPT

    url = 'http://cbdm-01.zdv.uni-mainz.de/~mschaefer/hippie/queryHIPPIE.php?'

    error_messages = {
        '400':' Bad Request: The request was invalid.',
        '414': 'Request-URI Too Long: The request URI is too long.',
        '401': 'Unauthorized: Authentication failed.',
        '404': 'Not Found: The requested resource does not exist.',
        '500': 'Internal Server Error: The server encountered an error.'
    }
    # print (query_list)

    query=';'.join(query_list)

    parameters = {
        'proteins':query,
        'layers': '1', #layers = 0 to query interactions within the input set or 1 to query interactions between the input set and HIPPIE (optional, default = 1)
        'conf_thres': confidence_score,
        'out_type': 'conc_file'
    }

    # else:
    #   for i in range(len(query_list)):
    #     batch_proteins = query_list[i:i+500]
    #     hippie_query_api(batch_proteins, confidence_score)


    response = requests.post(url,params=parameters) #make a query in HIPPIE using the uniprot ids from the file
    print ('\n Response:', response.status_code)
    if response.status_code == 200:

      soup = BeautifulSoup(response.content, 'html.parser')# Parse the HTML response to find the form
      form = soup.find('form', {'id': 'netQuery'})

      if form:
        # Extract form action URL and hidden input fields
        action_url = form.get('action')
        form_data = {input_tag.get('name'): input_tag.get('value') for input_tag in form.find_all('input')}
        # print ('\n form_data:', form_data)
        # print ('\n action_url:', action_url)
        full_action_url = requests.compat.urljoin(response.url, action_url)

        print(f"Submitting the next form to: {full_action_url}")

    # Step 4: Submit the form to download the file
        download_response = requests.post(full_action_url, data=form_data, stream=True)
        print ('Download HIPPIE results: ', response.url) #this is a link that downloads the tsv file when it is clicked - i suppose that it is the same link that they sent to us via email
        print ('Download_response:', download_response)

        if download_response.status_code  == 500: #if form not found
          results_content = "No form found on the page."
          with open (f'proteins_error_{hippie_query_api.counter}', 'w') as file:
            file.write(query)
          print("\n No form found on the page.")
        else:
          results_content = download_response.content.decode('utf-8')
          print("\n Form submitted successfully!")

    elif str(response.status_code) in error_messages.keys() :
      print ('\n %s: %s' % (response.status_code, error_messages[str(response.status_code)]))

    else: #if status code != 200 and there is not such response in error messages
      print ('\n Error:', response.status_code)

    # filename = 'hippie_results.tsv'
    #if I create a csv file to save my results, again it saves the query uniprot ids in an HTML format
    # urlretrieve(new_url, filename)
    with open(filename, 'w') as file:
        file.write(results_content)


    # df = pd.DataFrame(download_response.content.decode('utf-8'))
    # # df.to_csv(filename, index=False, sep='\t')
    # print (df)


    print (f'\n The file {filename} has been saved!')

    # read_hippie_results(filename)

def read_hippie_results (input_filename, output_filename):

  header = ['uniprot id 1', 'entrez gene id 1', 'gene name 1', 'uniprot id 2', 'entrez gene id 2', 'gene name 2', 'score'] #these are the columns of the conc file that hippie db returns
  index = 0
  #in case that a protein is not found the message "didn't find (.*) in the database, skipped it" appears at the top of the file before the results that start from header. So with the code below I try to obtain both the proteins not found and the results from HIPPIE

  with open(input_filename, 'r') as file:
    reader = csv.reader(file, delimiter='\t')
    # print (list(reader))
    for i, row in enumerate(reader):
      if row != header:
        if row != []:
          protein = ((row[0]))# if i write print (row) it will print a list, so in this way it prints a str with a message with the protein which was not found
          pattern = r"didn't find (.*) in the database, skipped it"

          matches = re.finditer(pattern, protein)
          for m in matches:
              protein = (m.group(1)) #m.group is a string

          with open ('not_found_proteins.txt', 'a') as file:
            if protein not in 'not_found_proteins.txt':
              file.write(f'%s\n' % (protein))
      else:
        index = i
        # print (index)
        break

  hippie_interactions_df = pd.read_table(input_filename, sep ='\t', skiprows = index)
  print (hippie_interactions_df.head().to_string())
  print (hippie_interactions_df.columns.values)


  hippie_interactions_df.dropna(inplace = True, subset = ['uniprot id 1', 'uniprot id 2']) #remove any rows with 'nan' values - note: 'nan' is considered as float

  #Prepare the columns for the creation of a new csv file
  confidence_values_list = list(hippie_interactions_df.loc[:, 'score'])
  interactor_A_list_uniprot_ids = list(hippie_interactions_df.loc[:, 'uniprot id 1']) #discuss why i have chosen
  interactor_B_list_uniprot_ids = list(hippie_interactions_df.loc[:, 'uniprot id 2'])

  print ('\n Interactor A uniprot ids:', len (interactor_A_list_uniprot_ids))
  print (interactor_A_list_uniprot_ids)
  print ('\n Interactor B uniprot ids:', len (interactor_B_list_uniprot_ids))
  print (interactor_B_list_uniprot_ids)

  hippie_protein_pairs = []

  for a, b in zip (interactor_A_list_uniprot_ids, interactor_B_list_uniprot_ids):
    pair = (a, b) #just a note here: if i remove the pair (b,a) which is the same with (a,b) maybe i lose biderectional associations - useful (?) for networks
    if pair not in hippie_protein_pairs:
      hippie_protein_pairs.append(pair)

  #then i convert the uniprot ids to the equivalent gene names - just a note: The uniprot ids here are in the form of 'B2L11_HUMAN' and not accession numbers
  # interactor_A_list_genes = (protein_to_gene_conversion(interactor_A_list_uniprot_ids)['Gene'])

  dict_genes = protein_to_protein_accession_conversion(interactor_A_list_uniprot_ids + interactor_B_list_uniprot_ids)
  # interactor_B_dict_genes = uniprot_id_mapping(interactor_B_list_uniprot_ids)

  hippie_gene_pairs = []
  scores = []
  interactor_A_list_genes = []
  interactor_B_list_genes = []

  for pair in hippie_protein_pairs:
    protein_a = pair[0]
    protein_b = pair[1]
    index = hippie_protein_pairs.index(pair)

    if protein_a in dict_genes.keys() and protein_b in dict_genes.keys():
      gene_a = dict_genes[protein_a]['Acc']
      gene_b = dict_genes[protein_b]['Acc']

      if gene_a != 'None' and gene_b != 'None':
        pair = (gene_a, gene_b)

        if pair not in hippie_gene_pairs:
          hippie_gene_pairs.append(pair)
          interactor_A_list_genes.append(gene_a)
          interactor_B_list_genes.append(gene_b)
          scores.append(confidence_values_list[index])


  print ('\n Hippie gene pairs: ', len(hippie_gene_pairs))
  print ('\n Scores ', len(scores), '\n', scores)


  #create a new file that contains the interactors and the confidence value
  column_labels = ['Interactor A', 'Interactor B', 'Confidence Value']
  column_data = [interactor_A_list_genes, interactor_B_list_genes, scores]
  filename = output_filename
  new_csv_file(column_labels, column_data, filename, True)

  return hippie_gene_pairs #this is a list with tuples, each one represents a pair


def new_ppi_network(nodes, edges, filename):
    pairs_list = []
    for pair in edges:
        # print (pair)
      if (pair[0] in nodes and pair[1] in nodes) and (pair[0] != 'None' or pair[1] != 'None'):
        query_protein_index = nodes.index(pair[0])  # pair is a tuple
        reference_protein_index = nodes.index(pair[1])  # pair is a tuple; the

        pairs_list.append((query_protein_index, reference_protein_index))
    dt = np.dtype('int', 'int')
    new_matrix = np.array(pairs_list, dtype=dt)

    # print(pairs_list)

    g = ig.Graph(pairs_list)
    # print(g)

    g.vs['nodes'] = nodes
    # g.vs['localization'] = localization
    # g.es['protein'] = edges
    layout = g.layout("kamada_kawai", maxiter = 1000)
    # col_dict = {
    #     'cytosol': 'blue',
    #     'nucleus': 'green',
    #     'mitochondrion': 'yellow',
    #     'None': 'white'
    # }
    # g.vs['color'] = [col_dict[loc] for loc in g.vs['localization']]

    fig, ax = plt.subplots()
    ig.plot(g, layout=layout, vertex_label =g.vs['nodes'], vertex_size = 35, target=ax, bbox=(1000, 2000), margin=2250)

    # plt.figure(figsize=(10, 6))

    plt.savefig(filename)
    plt.show()

    print(f'\n The network {filename} has been created!')

def mobidb_query (query_list, input_type):

  mobidb_query.counter +=1

  url = 'https://mobidb.org/api/download_page?'

  error_messages = {
      '400':' Bad Request: The request was invalid.',
      '401': 'Unauthorized: Authentication failed.',
      '404': 'Not Found: The requested resource does not exist.',
      '414': 'Request-URI Too Long: The request URI is too long.',
      '500': 'Internal Server Error: The server encountered an error.'
  }

  query = ','.join(query_list)

  if input_type == 'genes':
    parameters = {
        'gene': query,
        'ncbi_taxon_id': '9606',
        'prediction-disorder-alphafold': 'exists'
    }

  elif input_type =='proteins':
    parameters = {
        'acc': query,
        'ncbi_taxon_id': '9606',
        'prediction-disorder-alphafold': 'exists',
    }

  response = requests.get (url, parameters)
  print ('\n Response:', response)
  results_dict = response.json() #this is a dictionary
  print ('\n Response json: ', response.json())


  entries_dictionary = {}
  uniprot_id_list = []

  if response.status_code == 200:

    for entry in (results_dict['data']): #results_dict['data'] is a list, entry is a dict

      # entry = response_dict['data'][i] #this is a dictionary
      if 'gene' in entry.keys():
        gene_name = entry['gene']
      uniprot_id = entry['acc']
      protein_length = entry['length']

      if 'reviewed' in entry.keys():
        if entry['reviewed'] == True:
          uniprot_id_list.append(uniprot_id)

        if 'prediction-disorder-alphafold' in entry.keys():
          # disprot = response_dict_data['curated-disorder-disprot'] #from the dictionary of dictionaries i select only the dictionary that is realated to disprot. the disprot dictionary is a dictionary of dictionaries
          alphafold_idr_content = entry['prediction-disorder-alphafold']['content_fraction'] # this is the value of the content_fraction dictionary and contains the idr content
          # print (response_dict_data['localization'])

          #I noticed that some genes have also synonyms, so if i take the gene_name as it is in some cases i also take the synonyms (e.g. 'VCPSynonyms=HEL-220' )

          patterns = [r'(.*)Synonyms(.*)', r'(.*)Name(.*)']

          for pattern in patterns:

            match = re.search(pattern, gene_name)  # Using `search` instead of `findall`
            if match:
              gene_name = match.group(1)

          entries_dictionary[uniprot_id] = {'Gene': gene_name, 'Alphafold IDR Content':float(f'{(alphafold_idr_content*100):2f}'), 'Length': protein_length}

        else:
          entries_dictionary[uniprot_id] = {'Gene': gene_name, 'Alphafold IDR Content': 0.0, 'Length': protein_length} # it means that is not available/not found yet, so i did not put 0.0

        if 'localization' in entry.keys():
          localization = entry['localization']
          entries_dictionary[uniprot_id]['Localization'] = localization
        else:
          entries_dictionary[uniprot_id]['Localization'] = 'None'
          # with ('mobidb_not_found.txt', 'a') as file:
          #   file.write(f'{uniprot_id} \n')
          #   # print ('\n', uniprot_id)

      else:
        print (uniprot_id, '\n')
        with open (f'no_reviewed_proteins_mobidb_({mobidb_query.counter}).txt', 'a') as txtfile:
          txtfile.write(f'{uniprot_id} \n')


  elif str(response.status_code) in error_messages.keys() :
    print ('\n %s: %s'% (response.status_code, error_messages[str(response.status_code)]))


  else:
    print ('\n Error:', response.status_code)

  print ('\n Mobidb Results:', entries_dictionary)

  # df = pd.DataFrame(entries_dictionary)
  # df.to_csv('mobidb_results.csv')

  return entries_dictionary

def protein_interactors_counter (interactors_list, pairs_list):

  '''
  This function takes as argument a list with protein pairs as tuples and returns a dictionary
  that has keys the protein ids and as values a list with proteins that each protein interacts with
  '''

  # counts_dict = {}  #this is a dict that has as keys the gene name and as values the number of interactors
  interactors_dict = {} #it has as key the protein id and as value a list with the protein ids of the proteins with which interacts
  # pair_genes = []
  # for pair in pairs_list:
  #   interactor_a = pair[0]
  #   interactor_b = pair[1]

  #   if interactor_a not in pair_genes:
  #     pair_genes.append(interactor_a)
  #   if interactor_b not in pair_genes:
  #     pair_genes.append(interactor_b)

  # converted_genes_dict = gene_to_protein_conversion(pair_genes)
  # print (converted_genes_dict)

  for pair in pairs_list:
    interactor_a = pair[0]
    interactor_b = pair[1]

    # interactor_a = converted_genes_dict[gene_a]['Protein']
    # interactor_b = converted_genes_dict[gene_b]['Protein']

    if interactor_a in interactors_list: # since the interactors list are the query proteins that i used as my initial datasets I know that there are no 'None' values so is unesessary to check for 'None' values
      if interactor_a not in interactors_dict.keys():
        interactors_dict[interactor_a] = [interactor_b]
      elif interactor_b not in interactors_dict[interactor_a]:
        interactors_dict[interactor_a].append(interactor_b)

    if interactor_b in interactors_list:
      if interactor_b not in interactors_dict.keys():
        interactors_dict[interactor_b] = [interactor_a]
      elif interactor_a not in interactors_dict[interactor_b]:
        interactors_dict[interactor_b].append(interactor_a)

    # if interactor_a != 'None' or interactor_b != 'None':
    #   if interactor_a in interactors_list and interactor_a not in interactors_dict.keys(): # i create a dict with keys only the proteins that i used in my queries,
    #   #and not any other proteins that were found in the intermediate
    #     interactors_dict[interactor_a] = [interactor_b]
    #   elif interactor_b not in interactors_dict[interactor_a]:
    #     interactors_dict[interactor_a].append(interactor_b)

    #   if interactor_b in interactors_list and interactor_b not in interactors_dict.keys():
    #     interactors_dict[interactor_b] = [interactor_a] #maybe it counts itself
    #   elif interactor_a not in interactors_dict[interactor_b]:
    #     interactors_dict[interactor_b].append(interactor_a)
    # else:


  no_interactors = list (set(interactors_list).difference(set(interactors_dict.keys()))) #these are proteins from the interactors list that were found not to have any pair
  for interactor in no_interactors:
    interactors_dict[interactor] = []


  print ('\n Total interactors: ', len(interactors_dict.keys()))
  print ('\n Proteins with no interactors: ', len(no_interactors))

  #Save my results in files
  protein_interactors_counter.counter += 1
  filename = f'not_interacting_proteins_{protein_interactors_counter.counter}.txt'
  with open (filename, 'w') as file:
    for protein in no_interactors:
      file.write(f'{protein} \n')

  print (f'\n The file {filename} has been saved!')

  new_csv_file (column_labels = ['Interactor A', 'Interactor B'],
                column_data = [interactors_dict.keys(), interactors_dict.values()],
                filename = f'interacting_proteins_{protein_interactors_counter.counter}.csv',
                index_status = True)

  return interactors_dict


def read_multiunired_results (filename, score, output_type, reverse):
  '''
    tsv_file: a table with the results from multiunired
    score: takes float values of 1.0, 0.5 or 0.0 indicating the presence of an association, paralogues, or no interaction
    reverse:
  '''
  read_multiunired_results.counter += 1 #track the number of times the function is called

  if re.search(r'\.csv$', filename): #check if the file ends with '.csv', so the $ symbol is used to define that this pattern is at the end of the given string
    df = pd.read_csv(filename, index_col = 'Unnamed: 0')

  elif re.search(r'\.tsv$', filename):
    df = pd.read_csv(filename, sep='\t', index_col = 'Unnamed: 0')

  elif re.search(r'\.xlsx$',filename):
    df = pd.read_excel(filename, index_col = 'Unnamed: 0')



  df = df.drop('Overall_Score', axis = 1, inplace = False) # axis 1 are the columns, inplace means to change the existing df, so since i need only the protein names i remove the last column that contains the total score
  if reverse == True:
    df = df.T
  #this file is a table where each row represents the query protein and each column the reference protein

  rows = list(df.index)  # saves the proteins in a list (the rows in MultiUnired analysis contain the proteins from DisProt)
  columns = list(df.columns.values)

  print ('\n MultiUniRed Rows: ', len(rows))
  print ('\n MultiUniRed Columns: ', len(columns))

  #MultiUniReD rows and columns contain the proteins in this form 'Q68D10 (SPTY2D1)', so i use regex to take only the uniprot accession
  multiunired_rows = []
  multiunired_columns = []
  pattern = r'(.*) (.*)'

  if output_type == 'genes':
    index = 2
  elif output_type == 'proteins':
    index = 1

  for row in rows:
    match = re.search(pattern, row)
    if match:
      row = match.group(index) #if row=match.group(1) i take the protein name elif row=match.group(2) i take the gene name
    multiunired_rows.append(row)


  for column in columns:
    match = re.search(pattern, column)
    if match:
      column = match.group(index)
    multiunired_columns.append(column)


  scores_array = df.values.astype(float) #take all the values of the table
  interactions_matrix = np.nonzero(scores_array == score) #returns a tuple arrays, one for each dimension,
  # with the indices of the values that to the given condition (score)

  list_of_coordinates = list(zip(interactions_matrix[0],interactions_matrix[1])) # create a list of the coordinates by merging the two arrays--> we have a list of tuples
  #interacted[0] are the query proteins (rows), interacted[1] are the reference proteins (columns)

  total_multiunired_pairs = [] # a list to save the protein pairs
  protein_interactors_list = [] # a list to save all the interactors - pre-process for protein-to-gene conversion
  interactor_a_list = []
  interactor_b_list = []

  for coord in list_of_coordinates: #coord is a tuple
      # print (coord)
      query_protein_index = int(coord[0]) # coord[0] is a class 'numpy.int64' so i transform it into an int
      reference_protein_index = int(coord[1])

      #i take the index of the matrix that the score is equal to one, and i
      #use this index to find the gene at this position in the rows_genes list
      query_protein = multiunired_rows[query_protein_index]
      reference_protein = multiunired_columns[reference_protein_index]
      interactor_a_list.append(query_protein)
      interactor_b_list.append(reference_protein)

      pair = (query_protein, reference_protein)

      if query_protein not in protein_interactors_list:
        protein_interactors_list.append(query_protein)

      if reference_protein not in protein_interactors_list:
        protein_interactors_list.append(reference_protein)

      if pair not in total_multiunired_pairs:
          total_multiunired_pairs.append(pair)

  print ('\n Total protein pairs: ', len(total_multiunired_pairs))
  print (total_multiunired_pairs)

  print ('\n Total protein interactors:', len(protein_interactors_list))
  print (protein_interactors_list)

  new_csv_file(column_labels = ['Interactor A', 'Interactor B'], column_data = [interactor_a_list, interactor_b_list], filename = f'multiunired_interactions_{score}_({read_multiunired_results.counter}).csv', index_status = False)

  return total_multiunired_pairs

"""#Data Processing"""

tsv_file = 'uniprot_proteome.tsv'
df = pd.read_table(tsv_file, sep ='\t')
uniprot_ids = list(df['Entry'])
print (uniprot_ids)
print (len(uniprot_ids))

"""#HIPPIE QUERY"""

# uniprot_protein_genes_dict = protein_to_gene_conversion(uniprot_ids) #returns a list
print (disprot_ids)
print (uniprot_ids)

print (len(disprot_ids))
print (len(uniprot_ids))

all_proteins = uniprot_ids
print (len(all_proteins))

# for i, query_genes in enumerate(all_proteins):
  # print (query_genes)
hippie_query_api.counter = 0
total_items = len(all_proteins)
fetched_items = 0  # Counter to track progress
count_files = 0 #count the totla number of the created files

while fetched_items < total_items:
      # Calculate the next batch size
      remaining_items = total_items - fetched_items
      current_batch_size = min(batch_size, remaining_items)

      batch_proteins = all_proteins[fetched_items : fetched_items + current_batch_size]

      outfile_name = 'hippie_' + str(count_files+1) +'.tsv'
      hippie_query_api(batch_proteins, 0.63, outfile_name)
      # Simulate the data fetching (replace with your fetch function)
      # print(f"Fetching batch: {fetched_items + 1} to {fetched_items + current_batch_size}...")
      time.sleep(0.5)  # Simulating delay for each fetch; remove this line in actual fetching

      # Update the count of fetched items
      fetched_items += current_batch_size

      # Calculate and display progress
      progress_percentage = (fetched_items / total_items) * 100
      print(f"Progress: {fetched_items}/{total_items} ({progress_percentage:.2f}%)\n")
      count_files+=1



print ('\n Total files created:', count_files)
  # batch_proteins = all_proteins[i:i+500]

all_proteins = uniprot_ids
print (len(all_proteins))

protein_list = []
with open ('proteins_error_13.txt', 'r') as txtfile:
  for line in txtfile.readlines():
    protein_list = line.split(';')

print (protein_list)
print (len(protein_list))

protein_list_a = protein_list[0:200]
protein_list_b = protein_list[200:400]

hippie_query_api.counter = 0
outfile_name = 'hippie_13_b.tsv'
hippie_query_api(protein_list_b, 0.63, outfile_name)

files_list = ['hippie_13_a.tsv', 'hippie_13_b.tsv']
print (files_list)
df = pd.concat(map(pd.read_table, files_list), ignore_index=True)
print(df)

df.to_csv('hippie_13.tsv', sep='\t', index=False)

count_files = 209
total_hippie_pairs = []
excluded_files = [] #files that are excluded because they contain no results from HIPPIE analysis - this may occur because of an error (e.g. 500)

for i in range(14,209):
  print (i)
  input_filename = 'hippie_' + str(i+1) +'.tsv'
  output_filename = 'hippie_pairs_' + str(i+1) +'.csv'
  with open (input_filename, 'r') as tsvfile:
    file_content = tsvfile.read()
    # print ((file_content))
    if "No form found on the page." in file_content.strip() or 'No interactions passing the filter criteria' in file_content.strip(): #I could also just check if the file is empty
      excluded_files.append(input_filename)
      print ('\n No results returned in ', f'{input_filename}')
    else:
      hippie_pairs = (read_hippie_results(input_filename, output_filename))

  new_pairs = list(set(hippie_pairs).difference(set(total_hippie_pairs)))
  for pair in new_pairs:
    total_hippie_pairs.append(pair)

print ('\n Total Hippie pairs: ', len(total_hippie_pairs))

filename = 'hippie_44.tsv'
df = pd.read_table(filename)
print (len(df))

new_df = df[30000:]
new_df.to_csv('hippie_14_b.tsv', sep='\t', index=False)
input_filename = 'hippie_14_b.tsv'
output_filename = 'hippie_pairs_14_b.csv'
print (len(new_df))

with open (input_filename, 'r') as tsvfile:
    file_content = tsvfile.read()
    # print ((file_content))
    if "No form found on the page." in file_content.strip() or 'No interactions passing the filter criteria' in file_content.strip(): #I could also just check if the file is empty
      excluded_files.append(input_filename)
      print ('\n No results returned in ', f'{input_filename}')
    else:
      hippie_pairs = (read_hippie_results(input_filename, output_filename))

    new_pairs = list(set(hippie_pairs).difference(set(total_hippie_pairs)))
    for pair in new_pairs:
      total_hippie_pairs.append(pair)

print ('\n Total Hippie pairs: ', len(total_hippie_pairs))

files_list = ['hippie_pairs_14_a.csv', 'hippie_pairs_14_b.csv']
df = pd.concat(map(pd.read_csv, files_list), ignore_index=True)
df.to_csv('hippie_pairs_14.csv', index=False)

included_files = []
excluded_files = []
for i in range(1,count_files+1):
  filename = f'hippie_pairs_{i}.csv'
  if filename == True:
    filename.appen(included_files)
  else:
    excluded_files.append(filename)

print ('\n Included files: ', len(included_files))
print ('\n Excluded files: ', len(excluded_files))

print (excluded_files)

!ls hippie_pairs_*.csv > filenames.txt

files_list = []
with open ('filenames.txt', 'r') as txtfile:
  for line in txtfile.readlines():
    files_list.append(line.strip())

print (files_list)
print (len(files_list))

#below i concatinate all the files with the results from hippie

df = pd.concat(map(pd.read_csv, files_list), ignore_index=True)
print(len(df))

new_csv_file(column_labels = ['Interactor A', 'Interactor B', 'Confidence Value'], column_data = [df['Interactor A'], df['Interactor B'], df['Confidence Value']], filename = 'hippie_total_pairs_proteome.csv', index_status = False )

print (len(total_hippie_pairs))

#In case that i do not want to run again hippie query, i read the file that i saved with the results
df = pd.read_csv('hippie_total_pairs_all_proteins.csv')
# print (df.head().to_string())
total_hippie_pairs = []

interactor_a_list = df['Interactor A']
interactor_b_list = df['Interactor B']

for interactor_a, interactor_b in zip(interactor_a_list, interactor_b_list):

  pair = (interactor_a, interactor_b)
  if pair not in total_hippie_pairs:
    total_hippie_pairs.append(pair)
print (total_hippie_pairs)
print (len(total_hippie_pairs))

!zip 'hippie_results.zip' hippie*.tsv

"""#MobiDB

At this phase i just obtain all the information available in MobiDB for my query proteins (i.e., AlphaFold percentage, length)
"""

# all_proteins = total_multiunired_pairs + total_hippie_pairs
total_interactors_list = uniprot_ids
total_mobidb_dict = {}
# for i, query_genes in enumerate(all_proteins):
  # print (query_genes)
total_items = len(total_interactors_list)
batch_size = 500
fetched_items = 0  # Counter to track progress
count_run_times = 0 #count the totla number of the created files
mobidb_query.counter =0
while fetched_items < total_items:
      # Calculate the next batch size
      remaining_items = total_items - fetched_items
      current_batch_size = min(batch_size, remaining_items)

      batch_proteins = total_interactors_list[fetched_items:fetched_items + current_batch_size]
      print (batch_proteins)
      # outfile_name = 'mobidb_' + str(count_files+1) +'.tsv'
      mobidb_dict = mobidb_query(batch_proteins, 'proteins')
      total_mobidb_dict.update(mobidb_dict)
      # print (mobidb_dict)
      # print (total_mobidb_dict)
      #
      # print(f"Fetching batch: {fetched_items + 1} to {fetched_items + current_batch_size}...")
      time.sleep(0.5)  # Simulating delay for each fetch; remove this line in actual fetching

      # Update the count of fetched items
      fetched_items += current_batch_size

      # Calculate and display progress
      progress_percentage = (fetched_items / total_items) * 100
      print(f"Progress: {fetched_items}/{total_items} ({progress_percentage:.2f}%)\n")
      count_run_times+=1

print ('\n Total times ran:', count_run_times)
print ('\n Total proteins:', len(total_mobidb_dict))
  # batch_proteins = all_proteins[i:i+500]
print (total_mobidb_dict)

print (len(total_mobidb_dict))
print (count_run_times)



protein_interactors_counter.counter = 0
all_proteins = uniprot_ids
print (len(all_proteins))
hippie_interactors_dict = protein_interactors_counter (all_proteins, total_hippie_pairs) #this is a dict that has as keys the uniprot and as values a list with the uniprot ids of the proteins that interacts with
print (len(hippie_interactors_dict))
print (hippie_interactors_dict)

print (len(set(total_mobidb_dict.keys()).intersection(set(uniprot_ids+disprot_ids))))
print (len(set(total_mobidb_dict.keys()).difference(set(uniprot_ids+disprot_ids))))
print (len(set(uniprot_ids+disprot_ids).difference(set(total_mobidb_dict.keys()))))

print (len(set(total_mobidb_dict.keys()).intersection(set(multiunired_interactors_dict.keys()))))
print (len(set(total_mobidb_dict.keys()).difference(set(uniprot_ids+disprot_ids))))
print (len(set(uniprot_ids+disprot_ids).difference(set(total_mobidb_dict.keys()))))

#this is just to append the protein counts in the total mobidb dict
count_multi = 0
for protein in total_mobidb_dict.keys(): #i obtain only the reviewed proteins -so some proteins from the total interactors list are ommited

  if protein in hippie_interactors_dict.keys() :
    total_mobidb_dict[protein]['HIPPIE Protein Counts'] = len(hippie_interactors_dict[protein])
  # else:
  #   total_mobidb_dict[protein]['HIPPIE Protein Counts'] = 0

print (len(total_mobidb_dict))
print (total_mobidb_dict)
df=pd.DataFrame(total_mobidb_dict)
df = df.T # i reverse the dataframe to have as columns the keys of each dictionary (i.e. 'Gene', 'Length', 'Alphafold IDR Content) and as rows the uniprot ids
# print (df)
filename = 'mobidb_results.csv'
df.to_csv(filename)
print ('The file', f'{filename}', 'has been saved!')

count = 0
for protein in total_mobidb_dict.keys():
  if 'MultiUniReD Protein Counts' in total_mobidb_dict[protein].keys() and 'HIPPIE Protein Counts' in total_mobidb_dict[protein].keys():
    count += 1

print (count)

print (count_multi)

#In case that i do not want to open
df = pd.read_csv('mobidb_results.csv', index_col = 0)
# print (df.head().to_string())
total_mobidb_dict = {}

for protein in df.index:
  mobidb_dict= {}
  mobidb_dict['Gene'] = df.loc[protein,'Gene']
  mobidb_dict['Length'] = df.loc[protein,'Length']
  mobidb_dict['Alphafold IDR Content'] = df.loc[protein,'Alphafold IDR Content']
  mobidb_dict['Protein Counts'] = df.loc[protein,'Protein Counts']
  total_mobidb_dict.update({protein: mobidb_dict})

print (total_mobidb_dict)
print (len(total_mobidb_dict))

"""#Graphs

##Functions for Graphs and Statistic Analysis
"""

def scatter_plot (x_axis, y_axis, x_label, y_label, title, tredline):
  '''
    x_axis: list with the x-axis values
    y_axis: list with the y-axis values
    x_label: string with the x-axis label
    y_label: string with the y-axis label
    title: string with the title of the graph
    tredline: bool value that indicates if a trendline should be added to the graph
  '''
  plt.figure(figsize=(30, 20))
  plt.scatter(x_axis, y_axis)
  plt.xlabel(x_label, fontsize = 20)
  plt.ylabel(y_label, fontsize = 20)
  plt.title(title, fontsize = 20)
  plt.xticks(fontsize=15)
  plt.yticks(fontsize=15)
  if tredline == True:
    z = np.polyfit(x_axis, y_axis, 1)
    p = np.poly1d(z)
    plt.plot(x_axis, p(x_axis), color = 'purple', linewidth = 3, label = (p))
    plt.plot(x_axis, p(x_axis))
    print ('\n Trendline equation: ', p)
  plt.savefig(title + '.png')
  plt.show()


def linear_regression (x_axis, y_axis, x_label, y_label, title, tredline):

  x = np.array(x_axis)
  y = np.array(y_axis)
  x = x.reshape(-1, 1)
  model = LinearRegression()
  model.fit(x, y)
  y_pred = model.predict(x)
  y_mean = y.mean()
  SSregression = ((y_pred - y_mean)**2).sum()
  SStotal = ((y - y_mean)**2).sum()
  r_squared = SSregression / SStotal
  print (r_squared)

  # x_label = x_label
  # y_label = y_label
  # title = title
  scatter_plot (x, y_pred, x_label, y_label, title, tredline)

def histogram (data_list, bin_size, x_label, y_label, title):
  '''
    data_list: list with the data to be plotted
    bin_size: tuple with the range and the size of the bins (start, stop, step)
    x_label: string with the x-axis label
    y_label: string with the y-axis label
    title: string with the title of the graph
  '''
  # Define bin edges (e.g., for bins of 500)
  start = bin_size[0]
  stop = bin_size[1]
  step = bin_size[2]
  bins = range(start, stop, step) # Bins: 0-500, 501-1000, ..., 4501-5000
  # Create the histogram
  plt.figure(figsize=(30, 20))
  plt.hist(data_list, bins=bins, edgecolor = 'blue' )


  # Add labels and title
  plt.xlabel(x_label, fontsize=14)
  plt.ylabel(y_label, fontsize=14)
  plt.title(title, fontsize=16)
  plt.xticks(bins, fontsize=12, rotation = 30)
  plt.yticks(fontsize=12)

  # Save and show the plot
  plt.savefig(title + '.png')
  plt.show()

def venn_diagram (subsets, set_labels, set_colors, title):
  '''
    subsets: tuple with the number of elements in each subset,
    set_labels: tuple with the labels of each subset - labels are string
    set_colors: tuple with the colors of each subset - labels are string
    title: string with the title of the graph
  '''
  plt.figure(figsize = (4,4))
  v = venn2 (subsets = subsets,
            set_labels = set_labels,
            set_colors = set_colors,)
  plt.title (title)
  plt.savefig (title + '.png')

  plt.show()


def mann_whitney_test (x, y):

  '''
    x, y: lists with the data to be tested
  '''

  x = np.array(x_axis)
  y = np.array(y_axis)
  test = mannwhitneyu(x,y, use_continuity = False, alternative = 'two-sided' )

  pvalue = test.pvalue
  statistic = test.statistic

  print ('\n Mann-Whitney U Test')
  print (f'\n p-value: {pvalue:.2f}')
  print (f'\n statistic: {statistic}')

"""###Scatter Plots

####HIPPIE
"""

x_axis = []
y_axis = []
count_none = 0
count_idrs = 0
no_idr_hippie = []
total = len(total_mobidb_dict)

for protein in total_mobidb_dict.keys():
  if 'HIPPIE Protein Counts' in total_mobidb_dict[protein].keys():
    # protein_counts = total_mobidb_dict[protein]['Protein Counts']
    protein_counts = total_mobidb_dict[protein]['HIPPIE Protein Counts']
    idr_content = total_mobidb_dict[protein]['Alphafold IDR Content']

    if idr_content != 0.0: # or idr_content != float
      x_axis.append(idr_content)
      y_axis.append((protein_counts))
      count_idrs +=1
    else:
      no_idr_hippie.append(protein)
      count_none +=1

# total = count_idrs + count_none

print ('\n Total proteins with IDR content: (%s/%s)' % (count_idrs, total))
print ('\n Total proteins without IDR content: (%s/%s) \n' % (count_none, total))

scatter_plot (x_axis, y_axis, 'Alphafold IDR Content (%)', 'Number of Interactors', 'Alphafold IDR Content vs. Number of Interactors (Proteome)', True)

linear_regression(x_axis, y_axis, 'Alphafold IDR Content (%)', 'Number of Interactors', 'Alphafold IDR Content vs. Number of Interactors (HIPPIE, linear_regression)', False)

"""###HIPPIE

####Histogram
"""

#the following code blocks splits the data from total_mobidb_dict into idrs and no idrs
idr_content_list = [] #a list with the idr percentage
idr_protein_counts_list = [] # a list with the number of interactors for each protein
idr_length_list = [] # a list whith the length of each protein with idr

no_idr_protein_counts_list = [] # a list with the number of interactors for each protein without idr
no_idr_length = [] # a list with the length of each protein with no idr
count_none = 0
count_idrs = 0
total = len(total_mobidb_dict.keys())

for protein in total_mobidb_dict.keys():
  if 'HIPPIE Protein Counts' in total_mobidb_dict[protein].keys():
    protein_counts = total_mobidb_dict[protein]['HIPPIE Protein Counts']
    idr_content = total_mobidb_dict[protein]['Alphafold IDR Content']
    length = total_mobidb_dict[protein]['Length']

    if idr_content != 0.0 : # or idr_content != float
      idr_content_list.append(idr_content)
      idr_protein_counts_list.append(protein_counts)
      idr_length_list.append(length)
      count_idrs +=1
    else:
      no_idr_protein_counts_list.append(protein_counts)
      no_idr_length.append(length)
      count_none +=1

print ('\n IDR content: (%s/%s)' % (count_idrs, total))
print ('\n No IDR content: (%s/%s) ' %  (count_none, total))
# print ('\n Total proteins with IDR content: (%s/%s)' % (count_idrs, total))

"""####Average protein length of the proteins without IDR"""

sum = 0
avg = 0.0
min_protein_length = float ('inf')
max_protein_length = float ('-inf')
for i in range (len(no_idr_length)):
  protein_len = no_idr_length[i]
  # protein_2 = no_idr_length[i+1]
  min_protein_length = min(protein_len, min_protein_length)
  max_protein_length = max(protein_len, max_protein_length)
  sum+= protein_len
print ('\n Min protein:', min_protein_length)
print ('\n Max protein:', max_protein_length)
print ('\n Average:', sum/len(no_idr_length))

with open ('min_max_ids_no_idr_hippie.txt', 'w') as file:

  for protein in total_mobidb_dict.keys():
    length = (total_mobidb_dict[protein]['Length'])
    if length == min_protein_length:
      file.write (f'\n Min protein: {protein}')
    elif length == max_protein_length:
      file.write (f'\n Max protein: {protein}')

#Histogram of the frequency of the protein lenghts

histogram (no_idr_length, (0, max_protein_length, 50), 'Protein Length (aa)', 'Frequency', 'Histogram (NO IDR, proteome)')

"""####Average protein length of the proteins with IDR"""

sum = 0
avg = 0.0
min_protein_length = float ('inf')
max_protein_length = float ('-inf')
for i in range (len(idr_length_list)):
  protein_len = idr_length_list[i]
  min_protein_length = min(protein_len, min_protein_length)
  max_protein_length = max(protein_len, max_protein_length)
  sum+= protein_len
print ('\n Min protein:', min_protein_length)
print ('\n Max protein:', max_protein_length)
print ('\n Average:', sum/len(idr_length_list))

with open ('min_max_ids_idr_hippie.txt', 'w') as file:

  for protein in total_mobidb_dict.keys():
    length = (total_mobidb_dict[protein]['Length'])
    if length == min_protein_length:
      file.write (f'\n Min protein: {protein}')
    elif length == max_protein_length:
      file.write (f'\n Min protein: {protein}')

#Histogram of the frequency of the protein lenghts

histogram (idr_length_list, (0, max_protein_length, 50), 'Protein Length (aa)', 'Frequency', 'Histogram (IDR proteome)')

"""####Box Plot"""

#I noticed that the majority of proteins have length between 200 and 400 aa so I wanted to count the average number of interactors
sum_idr = 0
sum_no_idr = 0
avg_idr = 0.0
avg_no_idr = 0.0
total_idr = 0
total_no_idr = 0
idr_counts = []
no_idr_counts = []


for protein in total_mobidb_dict.keys():
  # length = (total_mobidb_dict[protein]['Length'])
  protein_counts = total_mobidb_dict[protein]['HIPPIE Protein Counts']
  idr_content = total_mobidb_dict[protein]['Alphafold IDR Content']

  # if length in range (200,401):

  if idr_content == 0.0:
    no_idr_counts.append(protein_counts)
    sum_no_idr+= protein_counts
    total_no_idr+=1

  else:
    idr_counts.append(protein_counts)
    sum_idr+= protein_counts
    total_idr+=1



avg_idr = sum_idr/total_idr
print (sum_idr)
print (total_idr)
print (f'\n Average idr: {avg_idr:.2f}')
avg_no_idr = sum_no_idr/total_no_idr
print (sum_no_idr)
print (total_no_idr)
print (f'\n Average no idr: {avg_no_idr:.2f}')

#box plots for idr
# print (idr_content_list)
# print (idr_counts)
plt.figure(figsize = (20,10))
bx = plt.boxplot([idr_counts, no_idr_counts], vert= True, labels = ['IDR content', 'No IDR content'] , showfliers = False, showmeans=True)
# plt.ylim(0,300

# plt.ylim(0,300)

plt.ylabel ('Protein Counts')
plt.title ('Average Number of Interactors')
plt.savefig('Average Number of Interactors (boxplot hippie).png')
# plt.boxplot(no_idr_counts, vert = True, sym = 'None')
plt.show()

mann_whitney_test (idr_counts, no_idr_counts)

help (plt.boxplot)

"""####Scatter Plot (Length Vs Number of interactors)

#####Proteins with IDR
"""

print (idr_protein_counts_list)
print (idr_length_list)

x_axis = idr_length_list
y_axis = idr_protein_counts_list
# for count in idr_protein_counts_list:
#   y_axis.append(log(count)) # its risky because if it comes with 0 values it will print a math error

scatter_plot (x_axis, y_axis, 'Protein Length (aa)', 'Number of Interactors', 'Protein Length vs. Number of Interactors (Proteome IDRs)', True)

linear_regression (x_axis, y_axis, 'Protein Length (aa)', 'Number of Interactors', 'Protein Length vs. Number of Interactors (IDRs, linear_regression)', False)

#Mann Whitney test

mann_whitney_test (x_axis, y_axis)

"""#####Proteins without IDR"""

x_axis = no_idr_length
y_axis = no_idr_protein_counts_list
# y_axis = []
# for count in no_idr_protein_counts_list:
#   y_axis.append(log(count)) # math error

scatter_plot (x_axis, y_axis, 'Protein Length (aa)', 'Number of Interactors', 'Protein Length vs. Number of Interactors (Proteome NO IDRs)', True)

linear_regression (x_axis, y_axis, 'Protein Length (aa)', 'Number of Interactors', 'Protein Length vs. Number of Interactors (NO IDRs, linear_regression)', False)

#Mann Whitney

mann_whitney_test (x_axis, y_axis)

"""#Case Studies"""

def network_nodes (gene_list, database, outfile):
  '''
  Gene: the gene name that i want to create the network (maybe it will be a list of genes)
  Database: this is a string 'hippie' or 'multiunired'
  Datasets list: this is a list that contains the pairs found from each database
  Outfile the name of the file with the interactors
  '''

  interactors_a = []
  interactors_b = []
  pairs = []
  if database == 'hippie':
    # pairs_list = total_hippie_pairs
    for gene in gene_list:
      for pair in total_hippie_pairs:
        if (pair[0] == gene or pair[1] == gene) and (pair[0] != 'None' or pair[1] != 'None'):

          interactors_a.append(pair[0])
          interactors_b.append(pair[1])
          pairs.append(pair)

    # new_ppi_network (gene_list,pairs, f'{gene}.png')
    new_csv_file(column_labels = ['Interactor A', 'Interactor B'], column_data = [interactors_a, interactors_b], filename = outfile, index_status = False)

  if database == 'multiunired':

    for gene in gene_list:
      # print (gene)
      for pair in total_multiunired_pairs:
        if pair[0] == gene or pair[1] == gene:
          interactors_a.append(pair[0])
          interactors_b.append(pair[1])


    if len(pairs) == 0:
      print ('No pairs found')


      # filename = 'hippie_pairs_' + str(gene_list) + '.csv'
    new_csv_file(column_labels = ['Interactor A', 'Interactor B'], column_data = [interactors_a, interactors_b], filename = outfile, index_status = False)

"""##MTOR"""

network_nodes (['MTOR'], 'hippie', 'MTOR.csv')

network_nodes (['MTOR'], 'multiunired', 'MTOR_multi.csv')

"""#IDR content 100%"""

hippie_full_idr = []
for protein in total_mobidb_dict.keys():
  if total_mobidb_dict[protein]['Alphafold IDR Content'] == 100.0 and total_mobidb_dict[protein]['HIPPIE Protein Counts']:

    hippie_full_idr.append(protein)

print (len(hippie_full_idr))

multi_full_idr = []
for protein in total_mobidb_dict.keys():
  if total_mobidb_dict[protein]['Alphafold IDR Content'] == 100.0 and total_mobidb_dict[protein]['MultiUniReD Protein Counts']:
    multi_full_idr.append(protein)

print (len(multi_full_idr))
print (multi_full_idr)

print (total_mobidb_dict['O60927'])

print (len(set(multi_full_idr).intersection(set(hippie_full_idr))))

common = list(set(hippie_full_idr).intersection(set(multi_full_idr)))

protein_name = []
gene = []
protein_counts_hippie = []
protein_counts_multi = []
for protein in common:
  protein_name.append(protein)
  gene.append(total_mobidb_dict[protein]['Gene'])
  if  'HIPPIE Protein Counts' in total_mobidb_dict[protein].keys():
    protein_counts_hippie.append(total_mobidb_dict[protein]['HIPPIE Protein Counts'])
  else:
    protein_counts_hippie.append('None')
  if  'MultiUniReD Protein Counts' in total_mobidb_dict[protein].keys():
    protein_counts_multi.append(total_mobidb_dict[protein]['MultiUniReD Protein Counts'])
  else:
    protein_counts_multi.append('None')

new_csv_file(column_labels = ['Protein Accession', 'Gene', 'HIPPIE Protein Counts', 'MultiUniReD Protein Counts'], column_data = [protein_name, gene, protein_counts_hippie, protein_counts_multi], filename = 'common_idr_100.csv', index_status = False)

print (hippie_interactors_dict['P51397'])

protein_to_gene_conversion (hippie_interactors_dict['P51397'])

pairs_list = []
interactors_list = []
for pair in total_multiunired_pairs:
  if pair[0] == 'P51397' or pair[1] == 'P51397':
    pairs_list.append(pair)
    if pair[0] not in interactors_list:
      interactors_list.append(pair[0])
    elif pair[1] not in interactors_list:
      interactors_list.append(pair[1])

protein_gene_dict = protein_to_gene_conversion (interactors_list)

for pair in pairs_list:
  interactor_a = pair[0]
  interactor_b = pair[1]

  gene_a = protein_gene_dict[interactor_a]['Gene']
  gene_b = protein_gene_dict[interactor_b]['Gene']

  pair = (gene_a, gene_b)
  print (pair)

for pair in total_hippie_pairs:
  if pair[0] == 'DBNDD1' or pair[1] == 'DBNDD1':
    print (pair)

print (total_mobidb_dict['Q9H9R9']['Length'])

network_nodes (['DBNDD1','TRIM32','SQSTM1', 'PIP4K2A','LMLN','CALCOCO1' ], 'hippie', 'DBNDD1 interactors hippie.csv')

network_nodes (['DBNDD1','TRIM32','SQSTM1', 'PIP4K2A','LMLN','CALCOCO1' ], 'multiunired', 'DBNDD1 interactors multi.csv')

"""##0% IDR Content"""

hippie_no_idr = []
for protein in total_mobidb_dict.keys():
  if total_mobidb_dict[protein]['Alphafold IDR Content'] == 0.0 and total_mobidb_dict[protein]['HIPPIE Protein Counts']:

    hippie_full_idr.append(protein)

print (len(hippie_full_idr))

multi_no_idr = []
for protein in total_mobidb_dict.keys():
  if total_mobidb_dict[protein]['Alphafold IDR Content'] == 0.0 and total_mobidb_dict[protein]['MultiUniReD Protein Counts']:

    multi_no_idr.append(protein)

print (len(multi_no_idr))

common = list(set(hippie_no_idr).intersection(set(multi_no_idr)))
print (common)

common = list(set(hippie_no_idr).intersection(set(multi_no_idr)))

protein_name = []
gene = []
protein_counts_hippie = []
protein_counts_multi = []
for protein in common:
  protein_name.append(protein)
  gene.append(total_mobidb_dict[protein]['Gene'])
  if  'HIPPIE Protein Counts' in total_mobidb_dict[protein].keys():
    protein_counts_hippie.append(total_mobidb_dict[protein]['HIPPIE Protein Counts'])
  else:
    protein_counts_hippie.append('None')
  if  'MultiUniReD Protein Counts' in total_mobidb_dict[protein].keys():
    protein_counts_multi.append(total_mobidb_dict[protein]['MultiUniReD Protein Counts'])
  else:
    protein_counts_multi.append('None')

new_csv_file(column_labels = ['Protein Accession', 'Gene', 'HIPPIE Protein Counts', 'MultiUniReD Protein Counts'], column_data = [protein_name, gene, protein_counts_hippie, protein_counts_multi], filename = 'common_idr_0.csv', index_status = False)

for protein in total_mobidb_dict.keys():
  if total_mobidb_dict[protein]['Length'] == 16:
    if total_mobidb_dict[protein]['Alphafold IDR Content'] != 0.0:

      # print (protein)
      gene = total_mobidb_dict[protein]['Gene']
      # interactors = total_mobidb_dict[protein]['Interactors']
      protein_counts = total_mobidb_dict[protein]['Protein Counts']
      print (protein)
      print (gene)
      print (protein_counts)

print ('This protein ')

network_nodes(['TRDJ1'], 'hippie', 'TRDJ1_hippie.csv')

network_nodes(['TRDJ1'], 'multi', 'TRDJ1_multi.csv')

for pair in total_hippie_pairs:
  if pair[0] == 'PIPK4K2A' or pair[1] == 'PIP4K2A':
    print (pair)

!zip 'BIO491_Project(Human Proteome).zip' *.*